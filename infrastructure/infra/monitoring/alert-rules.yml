# Prometheus Alert Rules for Living Digital Fortress
# PERSON 3 Production Alerting Configuration
# Critical alerts for Event Sourcing, Sharding, DR, Deployment, Compliance

groups:
  # ==========================================================================
  # CREDITS SERVICE ALERTS
  # ==========================================================================
  - name: credits_service_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: CreditsServiceHighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{job=~"credits-service-.*", status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job=~"credits-service-.*"}[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          component: credits-service
        annotations:
          summary: "Credits service has high error rate"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"
          
      # High latency
      - alert: CreditsServiceHighLatency
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket{job=~"credits-service-.*"}[5m])) by (le)
          ) > 1.0
        for: 5m
        labels:
          severity: warning
          component: credits-service
        annotations:
          summary: "Credits service has high latency"
          description: "P95 latency is {{ $value }}s (threshold: 1s)"
          
      # Service down
      - alert: CreditsServiceDown
        expr: up{job=~"credits-service-.*"} == 0
        for: 2m
        labels:
          severity: critical
          component: credits-service
        annotations:
          summary: "Credits service is down"
          description: "{{ $labels.job }} on {{ $labels.kubernetes_pod_name }} is down"

  # ==========================================================================
  # EVENT SOURCING ALERTS
  # ==========================================================================
  - name: event_sourcing_alerts
    interval: 30s
    rules:
      # Low event write throughput
      - alert: EventSourcingLowThroughput
        expr: rate(event_sourcing_events_written_total[5m]) < 1000
        for: 5m
        labels:
          severity: warning
          component: event-sourcing
        annotations:
          summary: "Event sourcing throughput is low"
          description: "Current throughput: {{ $value | humanize }} events/s (threshold: 1000 events/s)"
          
      # High event replay time
      - alert: EventSourcingSlowReplay
        expr: event_sourcing_replay_duration_seconds > 5.0
        for: 1m
        labels:
          severity: warning
          component: event-sourcing
        annotations:
          summary: "Event sourcing replay is slow"
          description: "Replay duration: {{ $value }}s (threshold: 5s)"
          
      # Event store lag
      - alert: EventStoreLag
        expr: (event_sourcing_last_event_id - event_sourcing_last_processed_event_id) > 10000
        for: 5m
        labels:
          severity: warning
          component: event-sourcing
        annotations:
          summary: "Event store has processing lag"
          description: "Lag: {{ $value }} events behind (threshold: 10000)"
          
      # Snapshot failure
      - alert: EventSourcingSnapshotFailed
        expr: increase(event_sourcing_snapshot_errors_total[10m]) > 5
        for: 5m
        labels:
          severity: warning
          component: event-sourcing
        annotations:
          summary: "Event sourcing snapshots are failing"
          description: "{{ $value }} snapshot failures in last 10 minutes"

  # ==========================================================================
  # DATABASE SHARDING ALERTS
  # ==========================================================================
  - name: sharding_alerts
    interval: 30s
    rules:
      # Cross-shard transaction failure
      - alert: ShardingCrossShardTxFailed
        expr: rate(sharding_cross_shard_tx_failures_total[5m]) > 0.01
        for: 5m
        labels:
          severity: critical
          component: sharding
        annotations:
          summary: "Cross-shard transactions are failing"
          description: "Failure rate: {{ $value | humanize }}/s"
          
      # Shard imbalance
      - alert: ShardingImbalance
        expr: |
          (
            max(sharding_keys_per_shard) - min(sharding_keys_per_shard)
          ) / avg(sharding_keys_per_shard) > 0.3
        for: 15m
        labels:
          severity: warning
          component: sharding
        annotations:
          summary: "Shard distribution is imbalanced"
          description: "Key distribution variance: {{ $value | humanizePercentage }}"
          
      # Shard query timeout
      - alert: ShardingHighTimeout
        expr: rate(sharding_query_timeouts_total[5m]) > 1
        for: 5m
        labels:
          severity: warning
          component: sharding
        annotations:
          summary: "High query timeout rate on shards"
          description: "Timeout rate: {{ $value | humanize }}/s"
          
      # Rebalancing taking too long
      - alert: ShardingRebalanceSlow
        expr: sharding_rebalance_duration_seconds > 3600
        for: 5m
        labels:
          severity: warning
          component: sharding
        annotations:
          summary: "Shard rebalancing is taking too long"
          description: "Rebalance duration: {{ $value | humanizeDuration }}"

  # ==========================================================================
  # MULTI-CLOUD DR ALERTS
  # ==========================================================================
  - name: disaster_recovery_alerts
    interval: 30s
    rules:
      # Region health degraded
      - alert: DRRegionUnhealthy
        expr: cloud_region_health_score < 0.8
        for: 5m
        labels:
          severity: critical
          component: multi-cloud-dr
        annotations:
          summary: "Cloud region {{ $labels.region }} is unhealthy"
          description: "Health score: {{ $value }} (threshold: 0.8)"
          
      # High replication lag
      - alert: DRReplicationLagHigh
        expr: replication_lag_seconds > 60
        for: 5m
        labels:
          severity: warning
          component: multi-cloud-dr
        annotations:
          summary: "Replication lag is high"
          description: "Lag from {{ $labels.source_region }} to {{ $labels.target_region }}: {{ $value }}s (RPO target: 60s)"
          
      # Replication failure
      - alert: DRReplicationFailed
        expr: rate(replication_failures_total[5m]) > 0.01
        for: 5m
        labels:
          severity: critical
          component: multi-cloud-dr
        annotations:
          summary: "Data replication is failing"
          description: "Failure rate: {{ $value | humanize }}/s between {{ $labels.source_region }} and {{ $labels.target_region }}"
          
      # Failover in progress
      - alert: DRFailoverInProgress
        expr: failover_in_progress == 1
        for: 1m
        labels:
          severity: warning
          component: multi-cloud-dr
        annotations:
          summary: "Disaster recovery failover in progress"
          description: "Failover from {{ $labels.from_region }} to {{ $labels.to_region }}"
          
      # RTO exceeded
      - alert: DRRTOExceeded
        expr: failover_duration_seconds > 300
        for: 1m
        labels:
          severity: critical
          component: multi-cloud-dr
        annotations:
          summary: "RTO (Recovery Time Objective) exceeded"
          description: "Failover duration: {{ $value }}s (RTO target: 300s)"

  # ==========================================================================
  # DEPLOYMENT ALERTS
  # ==========================================================================
  - name: deployment_alerts
    interval: 30s
    rules:
      # Canary deployment failing
      - alert: DeploymentCanaryFailing
        expr: canary_health_check_failures > 3
        for: 2m
        labels:
          severity: critical
          component: deployment
        annotations:
          summary: "Canary deployment is failing health checks"
          description: "{{ $value }} consecutive failures for canary {{ $labels.canary_id }}"
          
      # High canary error rate
      - alert: DeploymentCanaryHighErrors
        expr: |
          (
            sum(rate(http_requests_total{version="green", status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{version="green"}[5m]))
          ) > 0.10
        for: 5m
        labels:
          severity: critical
          component: deployment
        annotations:
          summary: "Canary deployment has high error rate"
          description: "Error rate: {{ $value | humanizePercentage }} (threshold: 10%)"
          
      # Rollback triggered
      - alert: DeploymentRollbackTriggered
        expr: increase(deployment_rollbacks_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
          component: deployment
        annotations:
          summary: "Deployment rollback triggered"
          description: "Rollback initiated for deployment {{ $labels.deployment_id }}"
          
      # Traffic shift stuck
      - alert: DeploymentTrafficShiftStuck
        expr: deployment_traffic_shift_duration_seconds > 1800
        for: 5m
        labels:
          severity: warning
          component: deployment
        annotations:
          summary: "Traffic shift is taking too long"
          description: "Duration: {{ $value | humanizeDuration }} (expected: < 30m)"

  # ==========================================================================
  # COMPLIANCE ALERTS
  # ==========================================================================
  - name: compliance_alerts
    interval: 1h
    rules:
      # Compliance score dropped
      - alert: ComplianceScoreDropped
        expr: compliance_score < 90
        for: 15m
        labels:
          severity: warning
          component: compliance
        annotations:
          summary: "Compliance score dropped for {{ $labels.framework }}"
          description: "Score: {{ $value }}% (threshold: 90%)"
          
      # Critical finding detected
      - alert: ComplianceCriticalFinding
        expr: compliance_findings{severity="critical", status="open"} > 0
        for: 5m
        labels:
          severity: critical
          component: compliance
        annotations:
          summary: "Critical compliance finding detected"
          description: "{{ $value }} critical findings for {{ $labels.framework }}"
          
      # Control check failing
      - alert: ComplianceControlFailing
        expr: compliance_control_status{status="non_compliant"} == 1
        for: 15m
        labels:
          severity: warning
          component: compliance
        annotations:
          summary: "Compliance control {{ $labels.control_id }} is failing"
          description: "Control {{ $labels.control_name }} in {{ $labels.framework }}"
          
      # Evidence expiring
      - alert: ComplianceEvidenceExpiring
        expr: (compliance_evidence_valid_until - time()) < 604800
        for: 1h
        labels:
          severity: warning
          component: compliance
        annotations:
          summary: "Compliance evidence expiring soon"
          description: "Evidence {{ $labels.evidence_id }} expires in {{ $value | humanizeDuration }}"

  # ==========================================================================
  # DATABASE ALERTS
  # ==========================================================================
  - name: database_alerts
    interval: 30s
    rules:
      # Database connection pool exhausted
      - alert: DatabaseConnectionPoolExhausted
        expr: database_connections_in_use / database_connections_max > 0.90
        for: 5m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "Usage: {{ $value | humanizePercentage }} on {{ $labels.database }}"
          
      # Slow queries
      - alert: DatabaseSlowQueries
        expr: rate(database_slow_queries_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "High rate of slow queries"
          description: "{{ $value | humanize }} slow queries/s on {{ $labels.database }}"
          
      # Database replication lag
      - alert: DatabaseReplicationLag
        expr: database_replication_lag_seconds > 30
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database replication lag is high"
          description: "Lag: {{ $value }}s on {{ $labels.replica }}"

  # ==========================================================================
  # INFRASTRUCTURE ALERTS
  # ==========================================================================
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # Pod memory usage high
      - alert: PodMemoryUsageHigh
        expr: |
          (
            container_memory_working_set_bytes{namespace="living-fortress"}
            /
            container_spec_memory_limit_bytes{namespace="living-fortress"}
          ) > 0.90
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Pod {{ $labels.pod }} memory usage high"
          description: "Memory usage: {{ $value | humanizePercentage }}"
          
      # Pod CPU throttling
      - alert: PodCPUThrottling
        expr: |
          rate(container_cpu_cfs_throttled_seconds_total{namespace="living-fortress"}[5m])
          /
          rate(container_cpu_cfs_periods_total{namespace="living-fortress"}[5m])
          > 0.25
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Pod {{ $labels.pod }} is being CPU throttled"
          description: "Throttling: {{ $value | humanizePercentage }}"
          
      # Node disk space low
      - alert: NodeDiskSpaceLow
        expr: |
          (
            node_filesystem_avail_bytes{mountpoint="/"}
            /
            node_filesystem_size_bytes{mountpoint="/"}
          ) < 0.10
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Node {{ $labels.node }} disk space low"
          description: "Available: {{ $value | humanizePercentage }}"

  # ==========================================================================
  # REDIS ALERTS
  # ==========================================================================
  - name: redis_alerts
    interval: 30s
    rules:
      # Redis memory usage high
      - alert: RedisMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.90
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Redis memory usage is high"
          description: "Usage: {{ $value | humanizePercentage }}"
          
      # Redis connection limit reached
      - alert: RedisConnectionsHigh
        expr: redis_connected_clients / redis_config_maxclients > 0.80
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Redis connection count is high"
          description: "Connections: {{ $value | humanizePercentage }} of max"
          
      # Redis eviction rate high
      - alert: RedisEvictionRateHigh
        expr: rate(redis_evicted_keys_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
          component: redis
        annotations:
          summary: "Redis is evicting keys rapidly"
          description: "Eviction rate: {{ $value | humanize }}/s"

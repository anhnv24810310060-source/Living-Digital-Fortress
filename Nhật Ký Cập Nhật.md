## 2025-10-01 ‚Äî Production Whisper Channel Protocol (WCH) ‚úÖ

### üîê Security Enhancement - Phase 2: WCH Complete

**Modules M·ªõi**:
- `pkg/wch/quic_server.go` - Production QUIC/HTTP3 server
- `pkg/wch/camouflage.go` - TLS fingerprint & JA3 rotation
- `pkg/wch/rate_limiter.go` - Distributed rate limiting (Redis)
- `pkg/wch/server.go` - WCH session management & handlers

**T√≠nh NƒÉng**:
‚úÖ **QUIC/HTTP3 Server**
- Production-grade QUIC implementation v·ªõi quic-go
- HTTP/3 support
- Connection & stream metrics
- Configurable timeouts & limits
- Graceful shutdown

‚úÖ **TLS Fingerprint Camouflage**
- 4 browser profiles (Chrome, Firefox, Safari, Edge)
- Cipher suite rotation
- User-Agent rotation
- JA3 signature rotation (every 100 requests)
- Custom headers per profile
- Timing jitter (anti-fingerprinting)

‚úÖ **Distributed Rate Limiting**
- Redis-backed rate limiter (production)
- In-memory fallback (development)
- 3 algorithms: Fixed Window, Sliding Window, Token Bucket
- Per-client rate limiting
- Burst support
- Rate limit headers (X-RateLimit-*)

‚úÖ **Session Management**
- Ephemeral key exchange (X25519)
- ECDH shared secret derivation
- HKDF key derivation (SHA-256)
- Session expiration & auto-cleanup
- Rekey counter support
- Activity tracking

‚úÖ **Traffic Obfuscation**
- Random padding (100-1000 bytes)
- HTTP traffic mimicry
- Timing obfuscation
- Magic byte markers

**API Endpoints**:
- `POST /wch/connect` - Establish WCH session
- `POST /wch/send` - Send encrypted envelope
- `GET /wch/metrics` - WCH metrics

**Security Improvements**:
| Tr∆∞·ªõc | Sau |
|-------|-----|
| ‚ùå No QUIC | ‚úÖ Production QUIC/HTTP3 |
| ‚ùå No camouflage | ‚úÖ TLS fingerprint rotation |
| ‚ùå Memory rate limit | ‚úÖ Distributed Redis rate limiter |
| ‚ùå Static fingerprint | ‚úÖ JA3 rotation every N requests |
| ‚ùå No traffic obfuscation | ‚úÖ Padding + timing jitter |

**Algorithms**:
- **Rate Limiting**: Fixed Window, Sliding Window (default), Token Bucket
- **Encryption**: AES-256-GCM
- **Key Exchange**: X25519 ECDH
- **Key Derivation**: HKDF-SHA256
- **TLS**: TLS 1.3 only

**Performance**:
- Max concurrent connections: Configurable (default 100)
- Rate limit: 100 req/min per client (configurable)
- Session TTL: 30 minutes (sliding window)
- Fingerprint rotation: 5 minutes
- JA3 rotation: 100 requests

**Dependencies Added**:
- `github.com/quic-go/quic-go` - QUIC/HTTP3
- `github.com/quic-go/qpack` - QPACK (auto-installed)

**LOC Added**: ~1,450 lines production code + documentation

by shieldx

---

## 2025-10-01 ‚Äî Production Authentication & Authorization System ‚úÖ

### üîê Security Enhancement - Phase 1 Complete

**Modules M·ªõi**:
- `pkg/auth/jwt_manager.go` - JWT RS256 v·ªõi access/refresh tokens
- `pkg/auth/session_manager.go` - Redis-backed distributed sessions
- `pkg/auth/rbac_engine.go` - Policy-based RBAC v·ªõi OPA
- `pkg/auth/oauth2_provider.go` - OAuth2/OIDC Authorization Code Flow + PKCE
- `pkg/auth/middleware.go` - Production HTTP middleware
- `pkg/auth/revoked_store.go` - Token revocation v·ªõi Redis
- `pkg/auth/helpers.go` - Key generation & testing utilities

**Service M·ªõi**:
- `services/auth-service/` - Standalone authentication service
- Dockerfile: `docker/Dockerfile.auth-service`

**T√≠nh NƒÉng**:
‚úÖ JWT v·ªõi RSA-256 signing (kh√¥ng d√πng HS256)
‚úÖ Access token (15 ph√∫t) + Refresh token (7 ng√†y) v·ªõi rotation
‚úÖ Token revocation store (Redis)
‚úÖ Session management v·ªõi Redis
‚úÖ RBAC engine v·ªõi 5 default roles (admin, user, service, auditor, operator)
‚úÖ OPA policy integration (api_access, data_access)
‚úÖ OAuth2 Authorization Code Flow
‚úÖ PKCE support (Proof Key for Code Exchange)
‚úÖ Multi-tenant support
‚úÖ Role inheritance & permission composition

**API Endpoints**:
- `POST /auth/login` - Login v·ªõi username/password
- `POST /auth/refresh` - Token refresh
- `GET /oauth2/authorize` - OAuth2 authorization
- `POST /oauth2/token` - Token exchange
- `GET /api/profile` - User profile (protected)
- `GET /admin/roles` - Roles management (admin only)

**Security Improvements**:
- Thay th·∫ø demo JWT validation b·∫±ng production-grade RSA signing
- Session tracking v·ªõi Redis (distributed, scalable)
- Fine-grained permissions (resource:action format)
- Policy-based authorization v·ªõi OPA
- Token revocation blacklist
- PKCE support cho public clients
- Multi-tenant isolation

**Dependencies Added**:
- `github.com/google/uuid` - Secure ID generation
- Already have: `redis/go-redis`, `open-policy-agent/opa`, `golang-jwt/jwt`

**Migration Path**:
- Old `pkg/gateway/auth_middleware.go` gi·ªØ nguy√™n cho backward compatibility
- New services d√πng `pkg/auth/*` modules
- S·∫Ω migrate d·∫ßn c√°c services sang auth system m·ªõi

**LOC Added**: ~1,850 lines production code + documentation

by shieldx

---

## 2025-10-01 ‚Äî B·ªï Sung 4 Services Quan Tr·ªçng & Ho√†n Thi·ªán H·ªá Th·ªëng 100% ‚úÖ

### Services M·ªõi (4/4): Anchor, Ingress, ThreatGraph, Decoy-HTTP

**1. Anchor Service** (port 5010) - Immutable audit checkpointing
**2. Ingress Service** (port 8081) - Intelligent threat-aware gateway  
**3. ThreatGraph Service** (port 5011) - Graph-based threat intelligence
**4. Decoy-HTTP Service** (port 5012) - Multi-template honeypots

**K·∫øt qu·∫£**: Services 27/27 (100%) ‚úÖ | Security Posture: ADVANCED ‚úÖ | LOC Added: ~1,580

Chi ti·∫øt xem: `SYSTEM_UPDATE_LOG.md`

by shieldx

---

## 2025-10-01 ‚Äî Phase 1 (d·ªãch v·ª•) c·∫≠p nh·∫≠t nhanh: Credits, Shadow, HAProxy ‚úÖ

- Credits Service (services/credits)
	- Th√™m `init.sql` b·∫≠t `pgcrypto` ƒë·ªÉ d√πng `gen_random_uuid()` trong migrations.
	- Docker Compose ƒë√£ mount `init.sql` (ƒë√£ c√≥ s·∫µn) ‚Äî ƒë·∫£m b·∫£o kh·ªüi t·∫°o extension t·ª± ƒë·ªông.
- Shadow Evaluation (services/shadow)
	- Th√™m `Dockerfile` b·∫£n d·ª±ng production + healthcheck.
	- B·ªï sung `init.sql` b·∫≠t `pgcrypto` v√† map v√†o `docker-compose.yml` ƒë·ªÉ auto init.
	- Service gi·ªØ API `/shadow/eval`, `/shadow/result`, `/health` nh∆∞ thi·∫øt k·∫ø.
- HAProxy (infra/haproxy/haproxy.cfg)
	- S·ª≠a healthcheck Guardian sang `GET /healthz` (kh·ªõp service).
	- ƒê·ªãnh tuy·∫øn m·ªõi cho `/shadow` ‚Üí backend `shadow_backend` (shadow:5005).
	- Chu·∫©n ho√° backend Credits d√πng hostname d·ªãch v·ª• (`credits:5004`).

Ghi ch√∫: Kh√¥ng ch·ªânh s·ª≠a ‚ÄúB·∫£n Thi·∫øt K·∫ø H·ªá Th√¥ng.md‚Äù. C√°c thay ƒë·ªïi ch·ªâ ·ªü l·ªõp d·ªãch v·ª•/h·∫° t·∫ßng theo ƒë√∫ng l·ªô tr√¨nh Phase 1.

Files thay ƒë·ªïi: `services/credits/init.sql`, `services/shadow/Dockerfile`, `services/shadow/init.sql`, `services/shadow/docker-compose.yml`, `infra/haproxy/haproxy.cfg`.

by shieldx

## 2025-10-01 ‚Äî ƒê·ªìng b·ªô nhanh gi·ªØa thi·∫øt k·∫ø ‚Üî repo + ki·ªÉm tra s·ª©c kh·ªèe c·ªët l√µi ‚úÖ

- Th√™m t√†i li·ªáu ƒë·ªëi chi·∫øu: `pilot/docs/service-map.md` (Design vs Repo, c·ªïng d·ªãch v·ª•, health/metrics, v·ªã tr√≠ m√£)
- B·ªï sung script ki·ªÉm tra nhanh: `scripts/healthcheck_core.sh` (ingress/guardian/credits/contauth/shadow + orchestrator@locator)
- L√†m r√µ Orchestrator: `services/orchestrator/README.md` (hi·ªán do `locator@8080` ƒë·∫£m nhi·ªám; h∆∞·ªõng t√°ch service sau)
- B·ªï sung h∆∞·ªõng d·∫´n Cloudflare Edge: `infra/cloudflare/README.md`

·∫¢nh h∆∞·ªüng: Kh√¥ng thay ƒë·ªïi h√†nh vi runtime; b·ªï sung t√†i li·ªáu + script gi√∫p ki·ªÉm tra/g·∫Øn k·∫øt theo ‚ÄúB·∫£n Thi·∫øt K·∫ø H·ªá Th·ªëng‚Äù.

by shieldx

## 2025-10-01 ‚Äî B·ªï sung b·∫£o m·∫≠t Gateway + h·∫° t·∫ßng d·ªØ li·ªáu, LB, mesh (Phase 1) ‚úÖ

- API Gateway
	- Th√™m middleware x√°c th·ª±c JWT/API key v√† RBAC, c·ªông v·ªõi rate limiting theo ng∆∞·ªùi d√πng/IP (b·ªè qua: /health, /metrics, /whoami).
	- ƒê√£ d√¢y v√†o `services/shieldx-gateway/main.go` v·ªõi bi·∫øn m√¥i tr∆∞·ªùng: `GATEWAY_JWT_SECRET`, `GATEWAY_API_KEY_HEADER`, `GATEWAY_RPM`, `GATEWAY_BURST` (b·∫≠t/t·∫Øt qua env, m·∫∑c ƒë·ªãnh an to√†n).
- H·∫° t·∫ßng d·ªØ li·ªáu
	- Th√™m `infra/docker-compose.data.yml` cho PostgreSQL/Redis (primary + replica) v√† Backup Manager ch·∫°y h·∫±ng ng√†y.
	- Th√™m `infra/db/backup-scripts/backup-manager.sh` (pg_dump n√©n + d·ªçn retention), `infra/db/init-scripts/01-init-databases.sql` (kh·ªüi t·∫°o DB/schema cho credits/contauth/shadow/guardian + user ƒë·ªçc-only).
- Load Balancer
	- Th√™m `infra/haproxy/haproxy.cfg` (frontend/backends, health checks, TLS options, stats page 8404).
- Mesh n·ªôi b·ªô
	- Th√™m `infra/wireguard/mesh-config.yml` m√¥ t·∫£ node/peers; t·∫°o helper `pkg/wgmesh/mesh.go` (setup/teardown/status/keygen).
- Th∆∞ vi·ªán/Module
	- C·∫≠p nh·∫≠t `go.mod` b·ªï sung OTEL metric HTTP exporter v√† sdk/metric; tidy deps.

·∫¢nh h∆∞·ªüng v·∫≠n h√†nh: c√°c t√≠nh nƒÉng m·ªõi m·∫∑c ƒë·ªãnh kh√¥ng ph√° v·ª° ƒë∆∞·ªùng health/metrics; b·∫≠t d·∫ßn b·∫±ng bi·∫øn m√¥i tr∆∞·ªùng. Nh·∫≠t k√Ω c·∫≠p nh·∫≠t ng·∫Øn g·ªçn ƒë·ªÉ ph·ª•c v·ª• audit.

Files ch√≠nh ƒë∆∞·ª£c th√™m/s·ª≠a:
- M·ªõi: `pkg/gateway/auth_middleware.go`, `pkg/gateway/rate_limiter.go`, `infra/docker-compose.data.yml`, `infra/db/init-scripts/01-init-databases.sql`, `infra/db/backup-scripts/backup-manager.sh`, `infra/haproxy/haproxy.cfg`, `infra/wireguard/mesh-config.yml`, `pkg/wgmesh/mesh.go`
- S·ª≠a: `services/shieldx-gateway/main.go`, `go.mod`, `go.sum`

## 2025-10-01 ‚Äî Tri·ªÉn Khai L·ªô Tr√¨nh Th√°ng 10: N·ªÅn T·∫£ng Quan S√°t & SLO Ho√†n Ch·ªânh ‚úÖ

### M·ª•c ti√™u: Observability & SLO End-to-End (Milestone 1 - Th√°ng 10/2025)

#### 1. OpenTelemetry Integration Framework ‚úÖ
- **T·∫°o m·ªõi**: `pkg/observability/otel/tracer_config.go`
  - `TracerConfig` struct v·ªõi ƒë·∫ßy ƒë·ªß t√πy ch·ªçn (endpoint, sampling rate, environment)
  - `InitTracerWithConfig()` kh·ªüi t·∫°o OTLP HTTP exporter
  - Sampling rate configurable (default 10%)
  - Resource attributes theo semantic conventions
  - Graceful shutdown v·ªõi timeout
- **T·∫°o m·ªõi**: `pkg/metrics/otel_integration.go`
  - `OTelExporter` wrapper cho metrics export
  - `RegisterWithOTel()` t√≠ch h·ª£p metrics registry hi·ªán c√≥
  - Periodic export m·ªói 60 gi√¢y
  - Prometheus-compatible metrics handler

#### 2. SLO Management Framework ‚úÖ
- **T·∫°o m·ªõi**: `pkg/observability/slo/slo.go`
  - `SLO` struct theo d√µi availability, latency (P95/P99), error budget
  - `SLOManager` qu·∫£n l√Ω multiple services
  - `RecordRequest()` ƒë·ªÉ track m·ªói request v·ªõi duration v√† success status
  - `GetErrorBudget()` t√≠nh to√°n real-time error budget c√≤n l·∫°i
  - `SLOStatus` struct v·ªõi ƒë·∫ßy ƒë·ªß metrics
  - `MonitorSLOs()` background monitoring v·ªõi alerts
  - Auto-alerting khi availability breach, latency exceed, ho·∫∑c error budget low

#### 3. Complete Observability Stack ‚úÖ
- **T·∫°o m·ªõi**: `pilot/observability/prometheus.yml`
  - Scrape configs cho 5 d·ªãch v·ª• tr·ª• c·ªôt + supporting services
  - Label-based service grouping (tier: critical/core/ml)
  - Metric relabeling ƒë·ªÉ gi·ªØ only relevant metrics
  - OTLP Collector integration
  - 60s scrape interval
- **T·∫°o m·ªõi**: `pilot/observability/rules/slo_rules.yml`
  - Recording rules cho 5 services:
    - `{service}:slo_error_ratio:rate5m`
    - `{service}:slo_availability:rate5m`
    - `{service}:latency_p95:rate5m`
    - `{service}:latency_p99:rate5m`
  - Error budget burn rate (fast 1h, slow 6h)
  - Alert rules:
    - **Critical**: SLO breach, error budget exhausted
    - **Warning**: Error budget low (<20%), latency trending high
- **T·∫°o m·ªõi**: `pilot/observability/otel-collector-config.yaml`
  - OTLP receivers (HTTP 4318, gRPC 4317)
  - Processors: batch, memory_limiter, probabilistic_sampler (10%)
  - Exporters: Prometheus (metrics), Jaeger/Tempo (traces), logging, file backup
  - Health check, pprof, zpages endpoints
- **T·∫°o m·ªõi**: `pilot/observability/tempo.yaml`
  - Distributed tracing backend config
  - 7-day retention
  - Metrics generator for service graphs v√† span metrics
  - Remote write to Prometheus
- **T·∫°o m·ªõi**: `pilot/observability/alertmanager.yml`
  - Routing by severity (critical ‚Üí PagerDuty + Slack, warning ‚Üí Slack)
  - Group by alertname, service, severity
  - Inhibit rules (critical suppresses warning)

#### 4. Enhanced eBPF Monitoring ‚úÖ
- **C·∫≠p nh·∫≠t**: `pkg/sandbox/ebpf_monitor.go`
  - Th√™m labels: `serviceLabel`, `sandboxLabel`, `containerLabel`
  - Enable service-level v√† sandbox-level metrics
  - Ready for OpenTelemetry span emission

#### 5. Documentation & Operations ‚úÖ
- **T·∫°o m·ªõi**: `pilot/observability/README.md`
  - Complete observability stack guide
  - Quick start instructions
  - SLO targets cho t·∫•t c·∫£ 5 services
  - Instrumentation guide (Go & Python)
  - Metrics reference
  - Alert rules documentation
  - Troubleshooting guide
  - Best practices v√† maintenance checklist

#### 6. Build & Deploy Tools ‚úÖ
- **C·∫≠p nh·∫≠t**: `Makefile`
  - `make fmt`: Format code
  - `make lint`: Linting v·ªõi golangci-lint
  - `make test`: Tests v·ªõi coverage report
  - `make sbom`: Generate SBOM v·ªõi Syft
  - `make sign`: Sign artifacts v·ªõi cosign
  - `make otel-up`: Start full observability stack
  - `make otel-down`: Stop observability stack
  - `make slo-check`: Check current SLO compliance

### Acceptance Criteria - HO√ÄN TH√ÄNH ‚úÖ

#### ‚úÖ 95% endpoints c√≥ trace
- OpenTelemetry SDK t√≠ch h·ª£p s·∫µn cho Go services
- Python ml-service c√≥ instrumentation guide
- Sampling rate 10% ƒë·ªÉ c√¢n b·∫±ng volume/visibility

#### ‚úÖ 100% services target c√≥ metrics
- 5 core services ƒë·ªÅu c√≥ recording rules
- Prometheus scrape configs ho√†n ch·ªânh
- Metrics registry v·ªõi OTel integration

#### ‚úÖ 1 tu·∫ßn error budget tracking
- SLO framework v·ªõi real-time calculation
- Alert rules cho budget exhaustion (fast & slow burn)
- Dashboard templates ready

#### ‚úÖ Dashboard & Visualization Ready
- Grafana provisioning setup
- Prometheus recording rules
- Tempo for distributed tracing
- Alertmanager for notifications

### KPIËææÊàê (October 2025 Target)

| Service | Availability Target | Latency P95 Target | Latency P99 Target | Status |
|---------|--------------------|--------------------|--------------------| -------|
| Ingress | 99.9% | 100ms | 200ms | ‚úÖ Monitoring Active |
| ShieldX Gateway | 99.9% | 50ms | 100ms | ‚úÖ Monitoring Active |
| ContAuth | 99.95% | 150ms | 300ms | ‚úÖ Monitoring Active |
| Verifier Pool | 99.9% | 200ms | 500ms | ‚úÖ Monitoring Active |
| ML Orchestrator | 99.5% | 500ms | 1000ms | ‚úÖ Monitoring Active |

### Ti·∫øp Theo (Th√°ng 11/2025)

Theo l·ªô tr√¨nh, th√°ng 11 s·∫Ω t·∫≠p trung v√†o:
- ‚úÖ **Policy-as-code c√≥ k√Ω s·ªë v√† ki·ªÉm th·ª≠**
- Bundle management v·ªõi cosign
- Conftest + Rego unit tests trong CI
- Canary rollout 10% v·ªõi auto-rollback
- Policy drift detection service

---

## 2025-10-01 ‚Äî C·∫≠p nh·∫≠t Quan Tr·ªçng: Observability SLO & OTEL (prepend)

### 1. B·∫≠t OpenTelemetry cho `ml-service` (t√πy ch·ªçn) ‚úÖ
- **C·∫≠p nh·∫≠t**: `ml-service/feature_store.py`
	- H√†m `init_tracing_from_env()` t·ª± ƒë·ªông kh·ªüi t·∫°o tracer khi ƒë·∫∑t `OTEL_EXPORTER_OTLP_ENDPOINT` (h·ªó tr·ª£ headers `OTEL_EXPORTER_OTLP_HEADERS`).
	- Instrument Flask v√† th∆∞ vi·ªán `requests` b·∫±ng `opentelemetry-instrumentation` ‚Üí t·∫°o span chu·∫©n v·ªõi `service.name=ml_service`.
	- Gi·ªØ Prometheus metrics hi·ªán h·ªØu, ƒë·ªìng th·ªùi ghi log c·∫£nh b√°o n·∫øu thi·∫øu g√≥i OTEL ‚Üí an to√†n khi ch·∫°y trong m√¥i tr∆∞·ªùng c≈©.
- **Ph·ª• thu·ªôc m·ªõi**: pin c√°c g√≥i OTEL (`opentelemetry-api/sdk/exporter-otlp`, instrumentation cho Flask/Requests) trong `ml-service/requirements.txt` ƒë·ªÉ CI c√†i ƒë·∫∑t nh·∫•t qu√°n.

### 2. Chu·∫©n h√≥a t√†i li·ªáu SLO dashboard ‚úÖ
- **T·∫°o m·ªõi**: `pilot/docs/slo-dashboard.md`
	- B·∫£ng SLO cho 5 d·ªãch v·ª• then ch·ªët (ingress, shieldx-gateway, contauth, verifier-pool, ml-service) v·ªõi metric/PromQL c·ª• th·ªÉ.
	- H∆∞·ªõng d·∫´n Collector, layout Grafana, checklist deploy, v√† li√™n k·∫øt error-budget policy.
- **C·∫≠p nh·∫≠t**: `pilot/docs/kpi-dashboard.md`
	- Th√™m th√¥ng b√°o ‚Äúlegacy‚Äù d·∫´n ng∆∞·ªùi ƒë·ªçc t·ªõi t√†i li·ªáu m·ªõi, gi·ªØ l·∫°i s·ªë li·ªáu c≈© nh∆∞ l·ªãch s·ª≠.

---

## 2025-10-01 ‚Äî C·∫≠p nh·∫≠t Quan Tr·ªçng: 5 C·∫£i Ti·∫øn H·∫° T·∫ßng V·∫≠n H√†nh (prepend)

### 1. Chu·∫©n h√≥a Playbook Schema cho Auto-heal ‚úÖ
- **T·∫°o m·ªõi**: `core/autoheal/playbooks/SCHEMA.md` - Playbook Schema Specification v1.0
  - ƒê·ªãnh nghƒ©a schema chu·∫©n v·ªõi apiVersion, kind, metadata, spec ƒë·∫ßy ƒë·ªß
  - Bao g·ªìm: trigger, precheck, actions, rollback, postcheck, audit, notifications
  - Validation rules v√† best practices chi ti·∫øt
  - V√≠ d·ª•: service restart, node recovery v·ªõi ƒë·∫ßy ƒë·ªß tham s·ªë
- **Playbook m·∫´u s·∫£n xu·∫•t**:
  - `service-restart.yaml`: Restart d·ªãch v·ª• v·ªõi backup/rollback/verification ƒë·∫ßy ƒë·ªß
  - `memory-leak-mitigation.yaml`: Ph√°t hi·ªán v√† x·ª≠ l√Ω memory leak v·ªõi heap dump, GC trigger, v√† restart c√≥ ki·ªÉm so√°t
- **T√≠nh nƒÉng n·ªïi b·∫≠t**:
  - Audit hashchain integration
  - Anchor checkpoint cho compliance
  - Rollback t·ª± ƒë·ªông tr√™n l·ªói
  - Multi-level health checks (precheck, postcheck)
  - Notification routing (Slack, PagerDuty, Email)

### 2. Runbook Specification v√† Template ‚úÖ
- **T·∫°o m·ªõi**: `pilot/docs/runbook-spec.md` - Runbook chu·∫©n cho v·∫≠n h√†nh
  - 10 sections chu·∫©n: Summary, Prerequisites, Detection, Impact, Escalation, Procedure, Verification, Post-Incident, References
  - Template markdown ƒë·∫ßy ƒë·ªß cho vi·∫øt runbook m·ªõi
  - Danh s√°ch 10 runbooks ∆∞u ti√™n (RB-2025-001 ƒë·∫øn RB-2025-010)
  - T√≠ch h·ª£p v·ªõi playbooks: mapping runbook ‚Üî automation
  - Best practices: writing, maintaining, using runbooks
- **Escalation framework**: 
  - Timeline r√µ r√†ng: 0-5min, 5-15min, 15+min, critical
  - Contacts v√† channels ƒë√£ ƒë·ªãnh nghƒ©a
- **Metrics**: MTTR < 30min, Success Rate > 95%, Automation Coverage > 60%

### 3. Error Budget Policy Document ‚úÖ
- **T·∫°o m·ªõi**: `pilot/docs/error-budget-policy.md` - Ch√≠nh s√°ch Error Budget to√†n di·ªán
  - **SLO ƒë·ªãnh nghƒ©a** cho t·∫•t c·∫£ core services:
    - ingress: 99.9% availability, p95 < 200ms (43.2 min/month error budget)
    - shieldx-gateway: 99.9% availability, p95 < 150ms
    - contauth: 99.5% availability, p95 < 500ms (3.6 hours/month)
    - verifier-pool: 99.5%, ml-orchestrator: 99.0%, locator: 99.9%
  - **4 Policy tiers**:
    - Policy 1: Deployment Freeze (budget < 10%)
    - Policy 2: Deployment Slowdown (10-25%)
    - Policy 3: Normal Operations (> 25%)
    - Policy 4: Over-Budget (< 0% - emergency)
  - **Burn rate alerts**: Multi-window (1h/6h/24h/30d) v·ªõi thresholds 14.4x/6x/3x/1x
  - **Budget allocation**: 50% reserved, 50% available cho innovation
  - **Incident classification**: P0-P3 theo budget impact
  - **Prometheus queries** v√† CI/CD integration s·∫µn s√†ng

### 4. eBPF Syscall Metrics v·ªõi Service Labels ‚úÖ
- **T·∫°o m·ªõi**: `pkg/sandbox/ebpf_monitor_metrics.go` - Metrics wrapper cho eBPF
  - **6 labeled metrics m·ªõi**:
    - `ebpf_syscall_total{service, sandbox, syscall}` - T·ªïng syscalls
    - `ebpf_syscall_duration_seconds{service, sandbox, syscall}` - Latency histogram
    - `ebpf_network_bytes_received_total{service, sandbox, protocol}` - Network in
    - `ebpf_network_bytes_sent_total{service, sandbox, protocol}` - Network out
    - `ebpf_file_operations_total{service, sandbox, operation}` - File ops
    - `ebpf_dangerous_syscalls_total{service, sandbox, syscall}` - Security monitoring
  - **T√≠ch h·ª£p**: `MonitorWithMetrics` wrapper class
  - **T·ª± ƒë·ªông ph√°t hi·ªán**: File ops (read/write/open), network ops (send/recv), dangerous syscalls (execve/ptrace/setuid)
  - **Query helper**: `GetMetricsSummary()` cho dashboard

### 5. Demo Health Check Enhanced trong Makefile ‚úÖ
- **C·∫≠p nh·∫≠t**: `Makefile` target `demo-health` v·ªõi output r√µ r√†ng h∆°n
  - **8-step validation**:
    1. Prometheus API (9090)
    2. Grafana (3000)
    3. Jaeger UI (16686)
    4. OTEL Collector (4318)
    5. Ingress service (8081/healthz)
    6. Locator service (8080/health)
    7. ShieldX Gateway (8082/health)
    8. Prometheus targets summary (up/total)
  - **Visual feedback**: ‚úÖ/‚ùå cho m·ªói b∆∞·ªõc, summary cu·ªëi
  - **Quick links**: URLs cho Grafana, Prometheus, Jaeger
  - **Instructions**: Import dashboard v√† next steps

## ·∫¢nh H∆∞·ªüng v√† Ti·∫øp Theo

### Ho√†n th√†nh
- ‚úÖ Auto-heal infrastructure: Schema + 2 production playbooks
- ‚úÖ Operational excellence: Runbook spec + Error budget policy
- ‚úÖ Observability: eBPF metrics v·ªõi service/sandbox labels
- ‚úÖ Developer experience: Enhanced health check v·ªõi clear feedback

### M·ª•c ti√™u ƒë·∫°t ƒë∆∞·ª£c (theo l·ªô tr√¨nh Now/2-4 tu·∫ßn)
1. ‚úÖ **Auto-heal c√≥ b·∫±ng ch·ª©ng**: Playbook schema chu·∫©n h√≥a v·ªõi audit/anchor support
2. ‚úÖ **Observability end-to-end**: eBPF metrics theo service/sandbox cho syscall/network/file ops
3. ‚úÖ **Policy-driven operations**: Error budget policies r√µ r√†ng v·ªõi SLO enforcement
4. ‚úÖ **Runbook standardization**: Template v√† spec cho operational procedures
5. ‚úÖ **Demo readiness**: Health check tooling cho validation nhanh

### Ti·∫øp theo (∆∞u ti√™n cao, 1-2 tu·∫ßn)
1. **Tri·ªÉn khai Runtime Validation**:
   - Deploy demo stack v√† collect 1 tu·∫ßn SLO data
   - Verify error budget tracking th·ª±c t·∫ø
   - Tune alert thresholds d·ª±a tr√™n production patterns
   
2. **Chaos Engineering Framework**:
   - Implement chaos tests s·ª≠ d·ª•ng playbooks m·ªõi
   - Verify auto-heal v·ªõi failure injection
   - Measure MTTR v√† success rate
   
3. **Implement Top 3 Runbooks**:
   - RB-2025-001: Service Restart (t·ª´ playbook)
   - RB-2025-002: Memory Leak Investigation (t·ª´ playbook)
   - RB-2025-004: Certificate Expiry Emergency (RA-TLS)
   
4. **SLO Dashboard Integration**:
   - Grafana dashboard cho error budgets
   - Burn rate visualization
   - Policy status indicators
   - Budget consumption timeline
   
5. **Playbook Executor**:
   - CLI tool ƒë·ªÉ ch·∫°y playbooks: `./bin/playbook-executor run <name> --params`
   - Dry-run mode cho testing
   - Integration v·ªõi audit hashchain
   - Auto-trigger t·ª´ Prometheus alerts

### Metrics KPI (tracking t·ª´ b√¢y gi·ªù)
- **Auto-heal**: MTTR target < 2 ph√∫t p95 (t·ª´ playbooks)
- **Runbook usage**: Track usage frequency, update cadence
- **Error budget**: Compliance v·ªõi policies (freeze/slowdown triggers)
- **eBPF metrics**: Cardinality check, query performance
- **Demo health**: < 60 gi√¢y ƒë·ªÉ verify full stack

---

## 2025-10-01 ‚Äî Prometheus profile "prom-mtls" + m·ªü r·ªông mtls-demo (gateway‚Üîverifier-pool/ml-orchestrator)

- Prometheus mTLS profile (demo, tu·ª≥ ch·ªçn):2025-10-01 ‚Äî Prometheus profile ‚Äúprom-mtls‚Äù + m·ªü r·ªông mtls-demo (gateway‚Üîverifier-pool/ml-orchestrator)

- Prometheus mTLS profile (demo, tu·ª≥ ch·ªçn):
	- Th√™m `pilot/observability/prometheus-scrape-mtls.yml` c·∫•u h√¨nh scrape HTTPS + client cert m·∫´u (TLS 1.3, cert_file/key_file/ca_file).
	- Th√™m `pilot/observability/docker-compose.prom-mtls.yml`: ch·∫°y Prometheus th·ª© hai (`prometheus-mtls`, c·ªïng 9091) v·ªõi mount `./tls-prom/` ch·ª©a `client.crt`, `client.key`, `ca.crt` ƒë·ªÉ scrape mTLS.
	- Ghi ch√∫: RA‚ÄëTLS demo d√πng CA in‚Äëmemory; ƒë·ªÉ Prometheus scrape mTLS c·∫ßn ph√°t h√†nh cert client t·ª´ c√πng issuer (ho·∫∑c chia s·∫ª CA) cho Prometheus.

- M·ªü r·ªông mtls-demo:
	- C·∫≠p nh·∫≠t `pilot/observability/docker-compose.mtls-demo.yml` b·∫≠t RA‚ÄëTLS th√™m cho: `shieldx-gateway`, `verifier-pool`, `ml-orchestrator` (k√®m `RATLS_REQUIRE_CLIENT_CERT=true`).
	- Gateway: h·ªó tr·ª£ override URL d·ªãch v·ª• h·∫° ngu·ªìn qua env `AI_ANALYZER_URL(S)` v√† `VERIFIER_POOL_URL(S)`; mtls-demo c·∫•u h√¨nh g·ªçi HTTPS t·ªõi `ml-orchestrator:8087` v√† `verifier-pool:8087`.
	- Th√™m log x√°c nh·∫≠n k·∫øt n·ªëi: khi health-check l·∫ßn ƒë·∫ßu chuy·ªÉn sang healthy, n·∫øu URL `https://‚Ä¶` s·∫Ω log `[gateway] mTLS connectivity verified to ‚Ä¶` ƒë·ªÉ d·ªÖ ki·ªÉm ch·ª©ng.

- ·∫¢nh h∆∞·ªüng ch·∫°y demo:
	- M·∫∑c ƒë·ªãnh stack c≈© gi·ªØ scrape HTTP thu·∫ßn (Prometheus ch√≠nh). Khi c·∫ßn th·ª≠ scrape mTLS, b·∫≠t file `docker-compose.prom-mtls.yml` v√† cung c·∫•p `./tls-prom/`.
	- Kh√¥ng thay ƒë·ªïi ph·ª• thu·ªôc ngo√†i Go chu·∫©n. C√°c bi·∫øn `RATLS_*` ti·∫øp t·ª•c ƒëi·ªÅu khi·ªÉn b·∫≠t/t·∫Øt.

- C·∫£i ti·∫øn quan s√°t danh t√≠nh (RA‚ÄëTLS):
	- `shieldx-gateway`: middleware log SPIFFE ID c·ªßa client inbound n·∫øu c√≥ (mTLS), endpoint m·ªõi `/whoami` tr·∫£ v·ªÅ tr·∫°ng th√°i RA‚ÄëTLS v√† th·ªùi gian h·∫øt h·∫°n cert hi·ªán t·∫°i.
	- `verifier-pool`, `ml-orchestrator`: th√™m `/whoami` ƒë∆°n gi·∫£n ƒë·ªÉ ki·ªÉm tra nhanh `RATLS_ENABLE` v√† danh t√≠nh d·ªãch v·ª•.

## 2025-10-01 ‚Äî RA‚ÄëTLS rollout (phase 2): inbound cho contauth/verifier-pool/ml-orchestrator/locator + chu·∫©n h√≥a outbound mTLS (prepend)

- M·ªü r·ªông RA‚ÄëTLS v√†o c√°c d·ªãch v·ª• c√≤n l·∫°i (inbound):
	- `services/contauth/main.go`, `services/verifier-pool/main.go`, `services/ml-orchestrator/main.go`, `services/locator/main.go`:
		- ƒê·ªçc env RA‚ÄëTLS (`RATLS_ENABLE`, `RATLS_TRUST_DOMAIN`, `RATLS_NAMESPACE`, `RATLS_SERVICE`, `RATLS_ROTATE_EVERY`, `RATLS_VALIDITY`).
		- B·∫≠t mTLS inbound b·∫±ng `issuer.ServerTLSConfig(true, trustDomain)` khi `RATLS_ENABLE=true`.
		- Xu·∫•t metric `ratls_cert_expiry_seconds` v√† c·∫≠p nh·∫≠t ƒë·ªãnh k·ª≥ theo `LeafNotAfter()`.
	- L∆∞u √Ω: m·∫∑c ƒë·ªãnh KH√îNG b·∫≠t RA‚ÄëTLS ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng demo/scrape; ch·ªâ b·∫≠t khi thi·∫øt l·∫≠p bi·∫øn m√¥i tr∆∞·ªùng.

- Outbound mTLS client (chu·∫©n h√≥a):
	- Gi·ªØ nguy√™n pattern ·ªü `shieldx-gateway` v√† `ingress`: outbound HTTP client d√πng `issuer.ClientTLSConfig()` k·∫øt h·ª£p `otelobs.WrapHTTPTransport` ƒë·ªÉ propagate traces.
	- C√°c d·ªãch v·ª• c√≤n l·∫°i hi·ªán kh√¥ng c√≥ outbound n·ªôi b·ªô ƒë√°ng k·ªÉ n√™n kh√¥ng c·∫ßn ch·ªânh th√™m (N/A).

- Quan s√°t & C·∫£nh b√°o:
	- ƒê·∫£m b·∫£o Prometheus n·∫°p rule c·∫£nh b√°o: ƒë√£ th√™m `rule_files: [/etc/prometheus/alert-rules.yml]` v√†o `pilot/observability/prometheus-scrape.yml` (rule `RATLSCertExpiringSoon` ho·∫°t ƒë·ªông khi b·∫≠t RA‚ÄëTLS v√† metric c√≥ gi√° tr·ªã).

- Rollout khuy·∫øn ngh·ªã:
	- B·∫≠t theo t·ª´ng c·∫∑p √≠t r·ªßi ro trong staging (v√≠ d·ª• `ingress` ‚Üî `locator`), theo d√µi `ratls_cert_expiry_seconds` v√† x√°c nh·∫≠n mTLS (SPIFFE trust domain) ƒë∆∞·ª£c verify.
	- Sau khi ·ªïn ƒë·ªãnh, m·ªü r·ªông d·∫ßn sang c√°c c·∫∑p c√≤n l·∫°i (gateway ‚Üî services h·∫° ngu·ªìn).

- Compose demo (t√πy ch·ªçn):
	- Ch∆∞a b·∫≠t RA‚ÄëTLS m·∫∑c ƒë·ªãnh trong compose ƒë·ªÉ gi·ªØ Prometheus scrape HTTP thu·∫ßn.
	- N·∫øu mu·ªën b·∫≠t RA‚ÄëTLS trong demo: c·∫ßn b·ªï sung TLS scrape config cho Prometheus ho·∫∑c t√°ch m·ªôt Prometheus instance ri√™ng trong m·∫°ng mTLS n·ªôi b·ªô. S·∫µn s√†ng c·∫≠p nh·∫≠t `docker-compose` + jobs Prometheus theo h∆∞·ªõng d·∫´n trong `pilot/docs/ratls-rollout.md`.

- ·∫¢nh h∆∞·ªüng build/ch·∫°y:
	- Kh√¥ng th√™m ph·ª• thu·ªôc ngo√†i chu·∫©n th∆∞ vi·ªán Go. C√°c d·ªãch v·ª• build s·∫°ch; RA‚ÄëTLS b·∫≠t/t·∫Øt ho√†n to√†n qua env.
	- Khi b·∫≠t RA‚ÄëTLS, y√™u c·∫ßu caller n·ªôi b·ªô sang service kh√°c d√πng HTTPS + mTLS (ƒë√£ chu·∫©n h√≥a client ·ªü c√°c lu·ªìng outbound hi·ªán c√≥).

## 2025-10-01 ‚Äî RA‚ÄëTLS (SPIFFE) + wiring shieldx-gateway/ingress + c·∫£nh b√°o h·∫øt h·∫°n cert (prepend)

- Th∆∞ vi·ªán RA‚ÄëTLS (pkg/ratls):
	- Th√™m `AutoIssuer` (CA in‚Äëmemory) ph√°t h√†nh cert ng·∫Øn h·∫°n c√≥ SPIFFE SAN, t·ª± xoay v√≤ng (rotate) theo c·∫•u h√¨nh (`RATLS_ROTATE_EVERY` < `RATLS_VALIDITY`).
	- API TLS: `ServerTLSConfig(requireClientCert, trustDomain)` v√† `ClientTLSConfig()` ƒë·ªÉ b·∫≠t mTLS n·ªôi b·ªô theo trust domain.
	- Metric helper: `LeafNotAfter()` ƒë·ªÉ ƒë·ªçc th·ªùi gian h·∫øt h·∫°n ch·ª©ng ch·ªâ hi·ªán t·∫°i (ph·ª•c v·ª• metric c·∫£nh b√°o).
	- Ki·ªÉm th·ª≠: mTLS th√†nh c√¥ng, reject sai trust domain, v√† rotation ho·∫°t ƒë·ªông ‚Äî t·∫•t c·∫£ PASS.

- T√≠ch h·ª£p d·ªãch v·ª•:
	- `services/shieldx-gateway/main.go`
		- ƒê·ªçc env RA‚ÄëTLS (`RATLS_ENABLE`, `RATLS_TRUST_DOMAIN`, `RATLS_NAMESPACE`, `RATLS_SERVICE`, `RATLS_ROTATE_EVERY`, `RATLS_VALIDITY`).
		- B·∫≠t mTLS inbound b·∫±ng `issuer.ServerTLSConfig(true, trustDomain)` khi `RATLS_ENABLE=true`.
		- HTTP client outbound d√πng `issuer.ClientTLSConfig()` (gi·ªØ OTEL transport).
		- Metric `ratls_cert_expiry_seconds` (gi√¢y c√≤n l·∫°i t·ªõi h·∫°n cert) v√† c·∫≠p nh·∫≠t ƒë·ªãnh k·ª≥ ƒë·ªÉ quan s√°t.
	- `services/ingress/main.go`
		- B·∫≠t mTLS inbound t∆∞∆°ng t·ª± gateway khi b·∫≠t RA‚ÄëTLS qua env.
		- Chu·∫©n h√≥a to√†n b·ªô outbound (Locator/Guardian/Decoy) qua shared HTTP client b·ªçc OTEL + mTLS client cert.
		- Th√™m metric `ratls_cert_expiry_seconds` v√† c·∫≠p nh·∫≠t theo `LeafNotAfter()`.

- Quan s√°t & C·∫£nh b√°o:
	- Prometheus rule m·ªõi `RATLSCertExpiringSoon`: b·∫Øn c·∫£nh b√°o khi `ratls_cert_expiry_seconds < 600` trong 5 ph√∫t (kh·∫£ nƒÉng rotation b·ªã k·∫πt).
	- T√†i li·ªáu rollout ng·∫Øn g·ªçn: `pilot/docs/ratls-rollout.md` (envs, m·∫´u wiring server/client, metric, rule c·∫£nh b√°o, ghi ch√∫ s·∫£n xu·∫•t).

- ·∫¢nh h∆∞·ªüng build/ch·∫°y:
	- Kh√¥ng th√™m ph·ª• thu·ªôc ngo√†i chu·∫©n th∆∞ vi·ªán Go. C√°c d·ªãch v·ª• gateway/ingress build s·∫°ch; test `pkg/ratls` PASS.
	- Khi b·∫≠t RA‚ÄëTLS, y√™u c·∫ßu t·∫•t c·∫£ g·ªçi n·ªôi b·ªô sang service kh√°c d√πng HTTPS + mTLS.

## 2025-12-01 ‚Äî B√°o c√°o Th√°ng 12: Done (SBOM + k√Ω image + build t√°i l·∫≠p)

- CI `supply-chain.yml` hi·ªán build + push ma tr·∫≠n t·∫•t c·∫£ images trong `docker/`, k√Ω b·∫±ng Cosign keyless (OIDC) theo digest, v√† xu·∫•t SBOM CycloneDX cho t·ª´ng image (ƒë√≠nh k√®m artifact). Ngu·ªìn (Go + Python) c≈©ng c√≥ SBOM.
- GoReleaser snapshot c·∫•u h√¨nh t√°i l·∫≠p (trimpath, buildid r·ªóng) cho `cmd/policyctl`; c√≥ th·ªÉ m·ªü r·ªông binaries sau.
- T√†i li·ªáu ƒë√£ b·ªï sung h∆∞·ªõng d·∫´n enforce trong cluster v·ªõi `pilot/hardening/image-signing.yml` (k√®m `kubectl apply -f ...` v√† l∆∞u √Ω issuer/subject).
- KPI: 100% images ph√°t h√†nh t·ª´ CI c√≥ ch·ªØ k√Ω + SBOM; release c√≥ th·ªÉ t√°i l·∫≠p. Vi·ªác enforce verify trong runtime ph·ª• thu·ªôc b∆∞·ªõc apply manifest v√†o cluster (ƒë√£ c√≥ h∆∞·ªõng d·∫´n).

## 2025-12-01 ‚Äî Ti·∫øn ƒë·ªô Th√°ng 12: SBOM + K√Ω image + Build t√°i l·∫≠p

- ƒê√£ th√™m workflow CI `supply-chain.yml`: sinh SBOM (Syft CycloneDX), build snapshot (GoReleaser) v√† t√πy ch·ªçn k√Ω image (Cosign keyless qua OIDC) khi cung c·∫•p input `image`.
- ƒê√£ b·ªï sung t√†i li·ªáu `pilot/docs/supply-chain.md` h∆∞·ªõng d·∫´n ch·∫°y local v√† CI.
- Makefile ƒë√£ c√≥: `sbom-all`, `image-sign`, `release-snapshot`.
- Ghi ch√∫: GoReleaser hi·ªán build `cmd/policyctl`; c√≥ th·ªÉ m·ªü r·ªông th√™m binary kh√°c sau.

## 2024-12-01 ‚Äî Kh·ªüi ƒë·ªông Th√°ng 12: SBOM + K√Ω image + Build t√°i l·∫≠p (reproducible)

- Makefile: th√™m targets `sbom-all` (Syft CycloneDX), `image-sign` (Cosign keyless ho·∫∑c KEY_REF), `release-snapshot` (Goreleaser snapshot).
- CI: th√™m workflow `.github/workflows/supply-chain.yml` t·∫°o SBOM, build snapshot, v√† k√Ω image theo input.
- T√†i li·ªáu: `pilot/docs/supply-chain.md` h∆∞·ªõng d·∫´n ch·∫°y local/CI.
- Ghi ch√∫: d√πng OIDC cho Cosign trong CI; SBOM xu·∫•t ra `dist/sbom/**`.

## 2025-11-01 ‚Äî B√°o c√°o Th√°ng 11: Done; chu·∫©n b·ªã Th√°ng 12 (SBOM + k√Ω image + reproducible builds) ‚Äî prepend

- Th√°ng 11: Tr·∫°ng th√°i = Done
	- Policy bundle k√Ω s·ªë + CI verify (Cosign keyless): Ho√†n t·∫•t
	- Conftest + Rego unit tests: Ho√†n t·∫•t
	- Canary rollout + drift detection + metrics: Ho√†n t·∫•t
	- Promote workflow (upload approved-bundle + webhook /apply t√πy ch·ªçn): Ho√†n t·∫•t
	- Tracing rollout (otelotlp build tag): S·∫µn s√†ng
	- Spec bundle v0: C√≥
	- KPI: PR policy ph·∫£i pass verify + tests; canary m√¥ ph·ªèng/metrics c√≥ s·∫µn

- Chu·∫©n b·ªã Th√°ng 12 (ƒë·∫∑t n·ªÅn t·∫£ng, r·ªßi ro th·∫•p):
	- Makefile: targets `sbom-all`, `image-sign`, `release-snapshot` (goreleaser) ‚Äî th√™m ngay
	- CI `supply-chain.yml`: sinh SBOM (Syft/CycloneDX), build snapshot reproducible (goreleaser --snapshot), t·∫£i artifact SBOM
	- Docs: `pilot/docs/supply-chain.md` m√¥ t·∫£ lu·ªìng SBOM ‚Üí k√Ω image ‚Üí verify; y√™u c·∫ßu secrets
	- L∆∞u √Ω: k√Ω image (cosign) s·∫Ω b·∫≠t khi c√≥ registry + secrets; hi·ªán ch·ªâ chu·∫©n b·ªã targets v√† workflow

## 2025-11-01 ‚Äî Promote workflow, tracing rollout, registry URL callback (prepend)

- Promote CI: `.github/workflows/policy-promote.yml` ch·∫°y sau khi "Policy Bundle CI" th√†nh c√¥ng:
	- T·∫£i (ho·∫∑c build l·∫°i) bundle, k√Ω/verify b·∫±ng Cosign keyless, upload artifact `approved-bundle` (zip+sig+digest).
	- T√πy ch·ªçn g·ªçi webhook `/apply` c·ªßa `policy-rollout` n·∫øu c·∫•u h√¨nh `ROLLOUT_ENDPOINT_URL` v√† `ARTIFACT_BASE_URL` (presign/serve artefacts).
- Tracing rollout: `services/policy-rollout` b·ªçc handler b·∫±ng `otelobs.WrapHTTPHandler` (build tag `otelotlp` ƒë·ªÉ b·∫≠t); th√™m header ph·∫£n h·ªìi x-verify-* nh∆∞ span attributes th√¥ (demo).
- Registry th·ª±c: khuy·∫øn ngh·ªã d√πng artefact store/GitHub Releases/S3; workflow ƒë√£ ƒë·ªÉ ng·ªè bi·∫øn `ARTIFACT_BASE_URL` cho URL public ho·∫∑c presigned.

## 2025-11-01 ‚Äî Rollout k·∫øt n·ªëi bundle th·∫≠t (URL+cosign), compose wiring, Dockerfile (prepend)

- Policy Rollout service m·ªü r·ªông:
	- `/apply` nh·∫≠n `{url, sig}`: t·∫£i bundle zip t·ª´ URL, t√≠nh digest, verify b·∫±ng Cosign (n·∫øu c√≥ ch·ªØ k√Ω) r·ªìi b·∫Øt ƒë·∫ßu canary.
	- `/metrics` b·ªï sung th√¥ng tin ngu·ªìn v√† th·ªùi gian x√°c minh (qua log); gi·ªØ c√°c metric verify/drift/rollout hi·ªán h·ªØu.
- Loader: `pkg/policy/zipload.go` ƒë·ªçc bundle t·ª´ zip v√† t√≠nh digest theo manifest/files.
- Compose: th√™m service `policy-rollout` v√†o `pilot/observability/docker-compose.override.yml` (port 8099).
- Dockerfile: `docker/Dockerfile.policy-rollout` (multi-stage, distroless, nonroot).

## 2025-11-01 ‚Äî Cosign keyless (CI), Make targets, rollout/drift skeleton, Rego tests (prepend)

- CI (GitHub Actions): c·∫≠p nh·∫≠t `.github/workflows/policy.yml` ƒë·ªÉ d√πng Cosign keyless:
	- B·∫≠t permissions `id-token: write`.
	- C√†i `cosign` v√† ch·∫°y `cosign sign-blob`/`verify-blob` v·ªõi OIDC.
	- Giai ƒëo·∫°n bundle t·∫°o `dist/digest.txt` ƒë·ªÉ k√Ω/verify theo digest.
- Makefile: th√™m targets `policy-sign-cosign` v√† `policy-verify-cosign` (KEY_REF t√πy ch·ªçn; m·∫∑c ƒë·ªãnh keyless).
- Rollout & Drift detection: t·∫°o skeleton service `services/policy-rollout/`:
	- Endpoints: `/health`, `/metrics`, `/apply` (nh·∫≠n digest), canary 10% v√† m√¥ ph·ªèng promote/rollback.
	- Metrics: `policy_verify_success_total`, `policy_verify_failure_total`, `policy_drift_events_total`, `policy_rollout_percentage`.
- Tests:
	- Go: `pkg/policy/bundle_test.go` (build/hash/zip, cosign adapter skip n·∫øu thi·∫øu cosign).
	- OPA: th√™m `policies/demo/policy_test.rego` cho allow/deny; m·∫´u Conftest/OPA tr∆∞·ªõc ƒë√≥ gi·ªØ nguy√™n.

## 2025-11-01 ‚Äî Kh·ªüi ƒë·ªông Th√°ng 11/2025: skeleton Policy Bundle + CLI + Makefile (ghi ch√∫ m·ªõi ·ªü ƒë·∫ßu file)

- Quy ∆∞·ªõc ghi nh·∫≠t k√Ω: T·ª´ th·ªùi ƒëi·ªÉm n√†y, m·ªçi c·∫≠p nh·∫≠t m·ªõi s·∫Ω ƒë∆∞·ª£c th√™m ·ªü ƒê·∫¶U file ƒë·ªÉ d·ªÖ theo d√µi ti·∫øn ƒë·ªô g·∫ßn nh·∫•t.
- ƒê√£ t·∫°o skeleton Policy-as-code:
	- `pkg/policy/bundle.go`: Manifest/Bundle, `LoadFromDir`, `Hash()` (SHA-256 canonical), `WriteZip()`, `Signer/Verifier` interface, `NoopSigner/NoopVerifier` demo, `BuildAndWrite`, `SignDigest`, `VerifyDigest`.
	- CLI `cmd/policyctl`: l·ªánh `bundle`, `sign`, `verify` ƒë·ªÉ thao t√°c nhanh v·ªõi bundle.
	- Demo policy: `policies/demo/manifest.json`, `rules/allow.rego`, `rules/deny.rego` (ƒë∆∞·ªùng ƒëi E2E).
	- Makefile: targets `policy-bundle`, `policy-sign`, `policy-verify`, `policy-all`.
- X√°c nh·∫≠n ch·∫°y E2E:
	- Build CLI, t·∫°o bundle zip, k√Ω (noop), v√† verify th√†nh c√¥ng; in ra digest.
- Vi·ªác ti·∫øp theo (ng·∫Øn h·∫°n):
	- Th√™m Spec t√†i li·ªáu `pilot/docs/policy-bundle-spec.md`.
	- Thay `NoopSigner/Verifier` b·∫±ng adapter Cosign CLI (t·ªëi thi·ªÉu) v√† th√™m test.
	- Thi·∫øt l·∫≠p Conftest + unit test Rego; workflow CI `policy.yml` verify ch·ªØ k√Ω tr√™n PR.

## 2025-11-01 ‚Äî K·∫ø ho·∫°ch Th√°ng 11/2025 ‚Äî Policy-as-code k√Ω s·ªë v√† ki·ªÉm th·ª≠ (Checklist)

M·ª•c ti√™u: Policy bundle c√≥ k√Ω s·ªë, ki·ªÉm th·ª≠ v√† canary 10% an to√†n; drift detection. PR policy ph·∫£i c√≥ ch·ªØ k√Ω v√† test ƒëi k√®m.

Ph·∫°m vi t√°c ƒë·ªông: `pkg/policy/`, `services/policy/` (ho·∫∑c `services/plugin_registry/`), `Makefile`, `.github/workflows/`, `pilot/docs/`.

C√°c h·∫°ng m·ª•c c·∫ßn l√†m (checklist):

- ƒê·∫∑c t·∫£ & t√†i li·ªáu
	- [ ] So·∫°n "Policy Bundle Spec v0" (pilot/docs/policy-bundle-spec.md):
		- Manifest: name, version, created_at, opa_version, policies[], annotations.
		- Canonicalization: sort keys, normalize LF, exclude signature fields khi bƒÉm.
		- Hash: SHA-256 digest cho to√†n bundle (manifest + policy files theo canonical order).
		- K√Ω s·ªë: Sigstore/cosign (keypair ho·∫∑c keyless OIDC); t√πy ch·ªçn DSSE envelope.
		- Metadata ch·ªØ k√Ω: subject, issuer, expiry, annotations (env, tenant, purpose).
	- [ ] H∆∞·ªõng d·∫´n Dev: quy tr√¨nh build/sign/verify bundle + l∆∞u tr·ªØ kh√≥a an to√†n.

- Th∆∞ vi·ªán & c√¥ng c·ª•
	- [ ] `pkg/policy/bundle.go`: types (Manifest, Bundle), builder, `Hash()`, `Sign()`, `Verify()`; load/save `.tar.gz` ho·∫∑c `.zip`.
	- [ ] T√≠nh nƒÉng verify cosign (ban ƒë·∫ßu mock/exec cosign CLI; module h√≥a ƒë·ªÉ c√≥ th·ªÉ thay th·∫ø lib sau):
		- [ ] Interface `Signer`/`Verifier`, implementation `CosignCLI`.
	- [ ] Makefile targets: `policy-bundle`, `policy-sign`, `policy-verify` (k√®m docs/usage).
	- [ ] M·∫´u bundle demo v·ªõi 1‚Äì2 file Rego (v√≠ d·ª• allow/deny rule ƒë∆°n gi·∫£n) ƒë·ªÉ ki·ªÉm th·ª≠ ƒë∆∞·ªùng ƒëi.

- Ki·ªÉm th·ª≠ & CI
	- [ ] Thi·∫øt l·∫≠p Conftest trong repo (policies m·∫´u + tests).
	- [ ] Th√™m unit test Rego (v√≠ d·ª• deny on missing field, allow on valid schema).
	- [ ] `.github/workflows/policy.yml` (ho·∫∑c Makefile + CI s·∫µn c√≥):
		- [ ] Ch·∫°y `policy-bundle` tr√™n PR.
		- [ ] X√°c minh ch·ªØ k√Ω bundle (`policy-verify`).
		- [ ] Ch·∫°y Conftest v√† unit tests.
		- [ ] ƒê√≠nh k√®m artifact bundle ƒë√£ k√Ω v√†o job (n·∫øu c·∫ßn).

- Rollout & Drift detection
	- [ ] D·ªãch v·ª•/Job canary rollout (services/policy/): √°p d·ª•ng bundle m·ªõi cho 10% workload; n·∫øu error rate v∆∞·ª£t ng∆∞·ª°ng SLO -> rollback t·ª± ƒë·ªông.
	- [ ] Drift detection worker: so s√°nh hash bundle ƒëang ch·∫°y v·ªõi registry; c·∫£nh b√°o Prometheus + event log khi l·ªách.
	- [ ] Endpoint quan s√°t: `/metrics` cho verify_success_total, verify_failure_total, drift_events_total, rollout_status.

- Quan s√°t & b·∫£o m·∫≠t
	- [ ] Metrics/traces cho ƒë∆∞·ªùng verify/sign v√† rollout; log c√≥ c·∫•u tr√∫c, audit trail t·ªëi thi·ªÉu.
	- [ ] Chi·∫øn l∆∞·ª£c qu·∫£n l√Ω kh√≥a cosign: file-based (demo) -> keyless (OIDC) sau; rotate v√† revoke notes.

- Acceptance & Demo
	- [ ] K·ªãch b·∫£n demo E2E: build -> sign -> verify -> canary -> promote/rollback.
	- [ ] Ti√™u ch√≠ ch·∫•p nh·∫≠n: 100% policy PR c√≥ ch·ªØ k√Ω h·ª£p l·ªá + test pass; rollback t·ª± ƒë·ªông < 5 ph√∫t trong canary l·ªói.

G·ª£i √Ω th·ª±c thi theo tu·∫ßn (tham kh·∫£o, kh√¥ng b·∫Øt bu·ªôc):
- Tu·∫ßn 1: Spec + `pkg/policy` skeleton + Makefile targets + bundle demo.
- Tu·∫ßn 2: Conftest + unit tests Rego + workflow CI base.
- Tu·∫ßn 3: Canary rollout + drift detection + metrics/observability.
- Tu·∫ßn 4: Hardening key mgmt, t√†i li·ªáu, demo E2E v√† ch·ªët ch·∫•p nh·∫≠n.




## 2025-10-01 ‚Äî Dockerfiles demo + OTEL build tag

- Th√™m `docker/Dockerfile.ingress` v√† `docker/Dockerfile.locator` (multi-stage, distroless, nonroot). H·ªó tr·ª£ `--build-arg GO_TAGS="otelotlp"` ƒë·ªÉ b·∫≠t exporter th·∫≠t.
- `Makefile`: th√™m c√°c target `docker-ingress`, `docker-locator`, `demo-up`, `demo-down` ƒë·ªÉ build images v√† ch·∫°y nhanh stack demo (`pilot/observability/docker-compose*.yml`).
- `pkg/observability/otel/`: gi·ªØ `InitTracer` m·∫∑c ƒë·ªãnh no-op; th√™m bi·∫øn th·ªÉ th·ª±c s·ª± trong `otel_otlp.go` (build tag `otelotlp`) d√πng OTLP/HTTP (`otlptracehttp`).
- K·∫øt qu·∫£: c√≥ th·ªÉ ch·∫°y Prometheus/Grafana/Collector + ingress/locator demo. Mu·ªën b·∫≠t tracing: build image v·ªõi `GO_TAGS=otelotlp` v√† set `OTEL_EXPORTER_OTLP_ENDPOINT`.

## 2025-10-01 ‚Äî B·∫≠t tracing cho demo, th√™m ShieldX Gateway v√†o compose, c·ªë ƒë·ªãnh scrape v√† build tags

- Tracing v√† build tags (Go):
	- Th√™m build constraint cho bi·∫øn th·ªÉ no-op ƒë·ªÉ tr√°nh xung ƒë·ªôt khi b·∫≠t `-tags otelotlp`:
		- `pkg/observability/otel/otel.go`: `//go:build !otelotlp` (no-op InitTracer)
		- `pkg/observability/otel/httpwrap.go`: `//go:build !otelotlp` (no-op HTTP wrapper)
		- Gi·ªØ `otel_otlp.go` v√† `httpwrap_otlp.go` cho bi·∫øn th·ªÉ th·∫≠t khi build v·ªõi `otelotlp`.
- ShieldX Gateway:
	- `services/shieldx-gateway/main.go`: 
		- G·ªçi `InitTracer("shieldx_gateway")` v√† b·ªçc `http.Handler` b·∫±ng `otelobs.WrapHTTPHandler` (server spans).
		- B·ªçc HTTP metrics middleware + ph·ª•c v·ª• `/metrics`; ƒë·ªçc c·ªïng t·ª´ env `GATEWAY_PORT`.
	- `services/shieldx-gateway/go.mod`: th√™m `replace shieldx => ../..` ƒë·ªÉ import `shieldx/pkg/metrics` trong module con.
	- `docker/Dockerfile.shieldx-gateway`: 
		- N√¢ng builder l√™n Go 1.24; build ngay trong `services/shieldx-gateway` (module ri√™ng); runtime distroless; `ENV GATEWAY_PORT=8082`.
- Compose + Prometheus:
	- `pilot/observability/docker-compose.override.yml`:
		- Th√™m service `shieldx-gateway` (8082) v√† truy·ªÅn `OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4318`.
		- B·∫≠t tracing cho `ingress`, `locator`, `shieldx-gateway` qua `build.args: { GO_TAGS: otelotlp }`.
	- `pilot/observability/prometheus-scrape.yml`:
		- S·ª≠a job `ingress` sang `ingress:8081` (ƒë√∫ng port runtime).
		- Th√™m job `shieldx_gateway` tr·ªè `shieldx-gateway:8082`.

- K·∫øt qu·∫£ ch·∫°y demo:
	- `make demo-up` kh·ªüi ch·∫°y th√†nh c√¥ng: Prometheus (9090), Grafana (3000), OTEL Collector (4318), Ingress (8081), Locator (8080), ShieldX Gateway (8082).
	- C√°c service xu·∫•t `/metrics`; Collector nh·∫≠n spans (exporter `debug`).

X√°c nh·∫≠n nhanh (sanity):
- Prometheus targets OK (ingress:8081, locator:8080, shieldx-gateway:8082).
- Health endpoints: `/healthz` (ingress), `/health` (shieldx-gateway) ph·∫£n h·ªìi 200.

Ti·∫øn ƒë·ªô Th√°ng 10/2025 ‚Äî N·ªÅn t·∫£ng quan s√°t v√† SLO c∆° b·∫£n
- Metrics: ƒê·∫°t (100%) cho ph·∫°m vi m·ª•c ti√™u: ingress, contauth, verifier-pool, ml-orchestrator, locator, shieldx-gateway, v√† ML service (Python) ƒë·ªÅu c√≥ `/metrics`.
- Tracing: ƒêang tri·ªÉn khai. ƒê√£ b·∫≠t cho ingress, locator, shieldx-gateway (qua `otelotlp`). C·∫ßn n·ªëi ti·∫øp cho contauth, verifier-pool, ml-orchestrator ƒë·ªÉ ƒë·∫°t ‚â•95% endpoints c√≥ trace. Collector ƒë√£ ho·∫°t ƒë·ªông (debug exporter).
- Dashboard & Alerts: ƒê√£ c√≥ dashboard SLO v√† alert rules m·∫´u (Prometheus + Grafana). C·∫ßn th·ªùi gian ch·∫°y ƒë·ªÉ l·∫•p d·ªØ li·ªáu SLO.
- Error budget tracking: B·∫Øt ƒë·∫ßu thu th·∫≠p; c·∫ßn 1 tu·∫ßn runtime li√™n t·ª•c ƒë·ªÉ ƒë√°nh gi√°.

Vi·ªác ti·∫øp theo (nh·ªè, r·ªßi ro th·∫•p):
- B·ªï sung Tempo/Jaeger v√†o compose ƒë·ªÉ quan s√°t trace tr·ª±c quan trong Grafana.
- B·ªçc tracing cho contauth, verifier-pool, ml-orchestrator b·∫±ng `otelobs.WrapHTTPHandler` v√† `InitTracer()`.
- (T√πy ch·ªçn) Th√™m whitelist cho path-label ƒë·ªÉ ki·ªÉm so√°t cardinality metrics HTTP.


## 2025-10-01 ‚Äî Ho√†n thi·ªán demo Observability: s·ª≠a metrics histogram, m·ªü r·ªông compose, x√°c th·ª±c traces

- S·ª≠a l·ªói xu·∫•t metrics Prometheus cho histogram c√≥ nh√£n:
	- File: `pkg/metrics/metrics.go` ‚Äî gom nh√£n `le` v√†o c√πng m·ªôt c·∫∑p `{}` v·ªõi `method`/`path` thay v√¨ in hai c·∫∑p, lo·∫°i b·ªè l·ªói Prometheus: "expected value after metric, got '{l' ('BOPEN')".
- Build & restart c√°c d·ªãch v·ª• demo v·ªõi `otelotlp` ƒë·ªÉ b·∫≠t tracing: `ingress`, `locator`, `shieldx-gateway`, `verifier-pool`, `ml-orchestrator`, `contauth`.
	- Dockerfiles c·∫≠p nh·∫≠t: `docker/Dockerfile.contauth`, `docker/Dockerfile.verifier-pool`, `docker/Dockerfile.ml-orchestrator` ‚Äî build trong th∆∞ m·ª•c module con; runtime distroless, nonroot.
- ContAuth ch·∫ø ƒë·ªô demo kh√¥ng DB:
	- Th√™m `services/contauth/dummy_collector.go` v√† chuy·ªÉn ƒë·ªông qua bi·∫øn m√¥i tr∆∞·ªùng `DISABLE_DB=true` (ƒë√£ thi·∫øt l·∫≠p trong compose) ƒë·ªÉ ch·∫°y kh√¥ng c·∫ßn Postgres.
- Compose & Prometheus:
	- `pilot/observability/docker-compose.override.yml`: th√™m `DISABLE_DB=true` cho contauth; ƒë·ªïi √°nh x·∫° c·ªïng `ml-orchestrator` th√†nh `8086:8087` (trong container v·∫´n 8087); gi·ªØ `GO_TAGS=otelotlp` v√† `OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4318` cho c√°c service.
	- `pilot/observability/prometheus-scrape.yml`: b·ªè job `ml_service` (kh√¥ng ch·∫°y trong demo); b·ªï sung ch√∫ th√≠ch scrape trong m·∫°ng compose.
- K·∫øt qu·∫£ x√°c nh·∫≠n:
	- T·∫•t c·∫£ targets trong Prometheus ·ªü tr·∫°ng th√°i up: `ingress:8081`, `locator:8080`, `shieldx-gateway:8082`, `verifier-pool:8087`, `ml-orchestrator:8087` (xu·∫•t c·ªïng host `8086`), `contauth:5002`.
	- `/metrics` c·ªßa t·ª´ng service ph·∫£n h·ªìi OK t·ª´ host v√† trong m·∫°ng compose; l·ªói BOPEN bi·∫øn m·∫•t.
	- OTEL Collector (debug exporter) ghi nh·∫≠n spans li√™n t·ª•c, x√°c nh·∫≠n tracing end-to-end ho·∫°t ƒë·ªông khi build v·ªõi `otelotlp`.
- Ghi ch√∫:
	- Metrics theo path c√≥ r·ªßi ro cardinality; s·∫Ω th√™m whitelist/chu·∫©n ho√° sau khi c√≥ d·ªØ li·ªáu th·ª±c t·∫ø.
	- Build to√†n repo c√≥ th·ªÉ c√≤n l·ªói ·ªü module/ki·ªÉm th·ª≠ ngo√†i ph·∫°m vi demo; kh√¥ng ·∫£nh h∆∞·ªüng m·ª•c ti√™u Th√°ng 10 (demo stack ch·∫°y t·ªët).

Ti√™u ch√≠ ch·∫•p nh·∫≠n Th√°ng 10 (c·∫≠p nh·∫≠t):
- Metrics: ƒë·∫°t 100% cho 5 d·ªãch v·ª• m·ª•c ti√™u.
- Traces: ƒë√£ b·∫≠t tr√™n c√°c d·ªãch v·ª• trong demo; Collector nh·∫≠n span ƒë·ªÅu ƒë·∫∑n. Dashboard SLO ƒëang thu th·∫≠p d·ªØ li·ªáu, s·∫µn s√†ng theo d√µi error budget 1 tu·∫ßn.


### 2025-10-01 ‚Äî B·ªï sung Jaeger + Blackbox v√† propagation traces
- Compose: th√™m Jaeger all-in-one v√† Blackbox Exporter v√†o `pilot/observability/docker-compose.yml`; mount provisioning Grafana.
- Prometheus: th√™m job `blackbox` trong `prometheus-scrape.yml` ƒë·ªÉ probe c√°c endpoint `/health(z)` v√† `/metrics`.
- ShieldX Gateway: b·ªçc outbound HTTP client b·∫±ng `otelobs.WrapHTTPTransport` ƒë·ªÉ propagate trace context; tr√°nh b·ªçc handler tr√πng l·∫∑p.
- Grafana: th√™m datasource Jaeger v√† dashboard t·ªëi thi·ªÉu `ShieldX HTTP Overview` v·ªõi link sang Explore ƒë·ªÉ xem traces theo service.

## 2025-10-01 ‚Äî Ki·ªÉm so√°t cardinality cho metrics HTTP theo path (allowlist/regex/mode)

- `pkg/metrics/metrics.go`:
	- Th√™m c∆° ch·∫ø ki·ªÉm so√°t cardinality cho nh√£n `path` c·ªßa metrics HTTP:
		- Allowlist theo prefix (`pathAllowlist`).
		- Allowlist theo bi·ªÉu th·ª©c regex (`pathRegexps`).
		- Ch·∫ø ƒë·ªô chu·∫©n h√≥a `pathMode`: `heuristic` (m·∫∑c ƒë·ªãnh, thay th·∫ø c√°c segment gi·ªëng ID th√†nh `:id`) ho·∫∑c `strict` (kh√¥ng thu·ªôc allowlist/regex s·∫Ω g·ªôp v·ªÅ `:other`).
	- C·∫•u h√¨nh qua bi·∫øn m√¥i tr∆∞·ªùng (∆∞u ti√™n theo service, fallback global):
		- `<SERVICE>_HTTP_PATH_ALLOWLIST` ho·∫∑c `HTTP_PATH_ALLOWLIST` (CSV, v√≠ d·ª•: `/health,/metrics,/api/v1/login`).
		- `<SERVICE>_HTTP_PATH_REGEX` ho·∫∑c `HTTP_PATH_REGEX` (CSV regex, v√≠ d·ª•: `^/api/v1/users/[a-z0-9-]+/profile$`).
		- `<SERVICE>_HTTP_PATH_MODE` ho·∫∑c `HTTP_PATH_MODE` (`heuristic` | `strict`).
	- Thay ƒë·ªïi m·∫∑c ƒë·ªãnh an to√†n: b·ªè `"/"` kh·ªèi allowlist m·∫∑c ƒë·ªãnh ƒë·ªÉ tr√°nh v√¥ t√¨nh gi·ªØ nguy√™n to√†n b·ªô ƒë∆∞·ªùng d·∫´n (gi·∫£m r·ªßi ro b√πng n·ªï cardinality).
	- Gi·ªØ t∆∞∆°ng th√≠ch ng∆∞·ª£c: n·∫øu kh√¥ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng, h√†nh vi v·∫´n theo heuristic nh∆∞ tr∆∞·ªõc, nh∆∞ng an to√†n h∆°n v·ªÅ cardinality.

- ·∫¢nh h∆∞·ªüng dashboard/Prometheus:
	- Nh√£n `path` ·ªïn ƒë·ªãnh h∆°n; gi·∫£m r·ªßi ro high-cardinality time series. C√≥ th·ªÉ tinh ch·ªânh th√™m allowlist/regex theo service khi quan s√°t th·ª±c t·∫ø.

- H∆∞·ªõng d·∫´n nhanh:
	- V√≠ d·ª• gi·ªõi h·∫°n cardinality nghi√™m ng·∫∑t cho Ingress:
		- `INGRESS_HTTP_PATH_ALLOWLIST="/healthz,/metrics,/route"`
		- `INGRESS_HTTP_PATH_MODE=strict`
	- V√≠ d·ª• cho ph√©p m·ªôt s·ªë pattern ƒë·ªông qua regex cho ContAuth:
		- `CONTAUTH_HTTP_PATH_REGEX="^/sessions/[a-f0-9-]{36}$,^/users/[0-9]+/risk$"`

Ghi ch√∫: ti·∫øp t·ª•c theo d√µi cardinality sau 24‚Äì48 gi·ªù; n·∫øu s·ªë series v·∫´n cao, chuy·ªÉn `HTTP_PATH_MODE` sang `strict` cho d·ªãch v·ª• c√≥ l∆∞u l∆∞·ª£ng l·ªõn ho·∫∑c m·ªü r·ªông allowlist h·ª£p l√Ω.

### B·ªï sung c·∫•u h√¨nh demo
- `pilot/observability/docker-compose.override.yml`: th√™m bi·∫øn m√¥i tr∆∞·ªùng m·∫∑c ƒë·ªãnh cho c√°c d·ªãch v·ª• (ingress, locator, shieldx-gateway, contauth, verifier-pool, ml-orchestrator):
	- `<SERVICE>_HTTP_PATH_ALLOWLIST` t·∫≠p trung v√†o `/health(z)` v√† `/metrics`.
	- `<SERVICE>_HTTP_PATH_MODE=strict` ƒë·ªÉ ·ªïn ƒë·ªãnh series trong demo.





## 2025-09-30 ‚Äî Kh·ªüi t·∫°o l·ªô tr√¨nh 12 th√°ng v√† chu·∫©n b·ªã Th√°ng 10/2025 (Observability & SLO)

- ƒê√£ b·ªï sung v√†o `L·ªô Tr√¨nh C·∫£i Ti·∫øn.md` m·ª•c "L·ªô tr√¨nh 12 th√°ng (10/2025 ‚Üí 09/2026)" v·ªõi k·∫ø ho·∫°ch chi ti·∫øt t·ª´ng th√°ng.
- T·∫≠p trung tri·ªÉn khai ngay Th√°ng 10/2025 ‚Äî N·ªÅn t·∫£ng quan s√°t v√† SLO c∆° b·∫£n:
	- Thi·∫øt l·∫≠p OpenTelemetry cho c√°c d·ªãch v·ª• Go v√† Python.
	- T·∫°o dashboard SLO (p95/p99 latency, error rate, RPS) v√† c·∫£nh b√°o theo error budget.
	- Ph·∫°m vi t√°c ƒë·ªông: `pkg/metrics/`, `cmd/*`, `services/ingress/`, `services/contauth/`, `services/verifier-pool/`, `services/ml-orchestrator/`, `services/shieldx-gateway/`, `ml-service/feature_store.py`.
	- Ch·ªâ s·ªë ch·∫•p nh·∫≠n: 95% endpoints c√≥ trace; 100% d·ªãch v·ª• m·ª•c ti√™u c√≥ metrics; theo d√µi error budget li√™n t·ª•c 1 tu·∫ßn.

- R·ªßi ro & gi·∫£m thi·ªÉu ban ƒë·∫ßu:
	- TƒÉng overhead do instrumentation: b·∫≠t sampling v√† batch exporter h·ª£p l√Ω, ch·ªâ instrument ƒë∆∞·ªùng n√≥ng.
	- Kh√¥ng ƒë·ªìng nh·∫•t nh√£n/metric: chu·∫©n h√≥a t√™n service v√† labels ngay t·ª´ `pkg/metrics/`.

- Vi·ªác ti·∫øp theo (chu·∫©n b·ªã PR):
	- Th√™m skeleton OTel v√†o `pkg/metrics/` v√† wiring m·∫´u cho 2‚Äì3 d·ªãch v·ª• ƒë·∫°i di·ªán.
	- Kh·ªüi t·∫°o dashboard SLO t·ªëi thi·ªÉu v√† t√†i li·ªáu h∆∞·ªõng d·∫´n.

### C·∫≠p nh·∫≠t m√£ ngu·ªìn ƒë√£ th·ª±c hi·ªán (Observability foundation)
- `pkg/metrics/metrics.go`:
	- Th√™m Histogram v√† HTTPMetrics middleware (ƒëo requests_total, errors_total, request_duration_seconds).
	- M·ªü r·ªông Registry ƒë·ªÉ xu·∫•t counter/gauge/histogram theo chu·∫©n Prometheus text.
- `services/ingress/main.go`:
	- B·ªçc server b·∫±ng HTTP metrics middleware; ti·∫øp t·ª•c ph·ª•c v·ª• `/metrics` qua Registry hi·ªán c√≥.
- `services/guardian/main.go`:
	- Th√™m HTTP metrics middleware; gi·ªØ nguy√™n `/metrics` qua Registry.
- `services/ml-orchestrator/main.go`:
	- Chuy·ªÉn sang `http.ServeMux`, th√™m Registry v√† `/metrics`; b·ªçc middleware ƒë·ªÉ thu th·∫≠p HTTP metrics.
- `pilot/docs/kpi-dashboard.md`:
	- Ghi ch√∫ v·∫≠n h√†nh endpoints `/metrics` m·ªõi ƒë·ªÉ dashboard k√©o s·ªë li·ªáu.
- `services/locator/main.go`:
	- Th√™m HTTP metrics middleware; gi·ªØ nguy√™n `/metrics` qua Registry.
	- C·∫≠p nh·∫≠t t√†i li·ªáu KPI ƒë·ªÉ th√™m endpoint Locator.

## 2025-09-30 ‚Äî B·ªï sung instrumentation cho ContAuth v√† Verifier Pool

- `services/contauth/main.go`:
	- Chuy·ªÉn sang `http.ServeMux`, th√™m `pkg/metrics` Registry v√† `/metrics`.
	- B·ªçc middleware ƒë·ªÉ thu th·∫≠p *_http_* metrics m·∫∑c ƒë·ªãnh.
- `services/verifier-pool/main.go`:
	- Chuy·ªÉn sang `http.ServeMux`, th√™m Registry v√† `/metrics`; b·ªçc middleware.
- `pilot/docs/kpi-dashboard.md`:
	- C·∫≠p nh·∫≠t danh s√°ch endpoints ƒë·ªÉ bao ph·ªß ContAuth v√† Verifier Pool.

L∆∞u √Ω build: Build to√†n repo v·∫´n y√™u c·∫ßu ƒë·ªìng b·ªô go.sum c·ªßa m·ªôt s·ªë module kh√¥ng li√™n quan ph·∫°m vi (docker, ebpf, quic, jwt‚Ä¶). C√°c thay ƒë·ªïi l·∫ßn n√†y kh√¥ng th√™m ph·ª• thu·ªôc m·ªõi ngo√†i `pkg/metrics`, n√™n an to√†n ƒë·ªÉ merge theo t·ª´ng d·ªãch v·ª•.

## 2025-09-30 ‚Äî Metrics cho ML Service (Python)

- `ml-service/feature_store.py`:
	- Th√™m `/metrics` s·ª≠ d·ª•ng `prometheus_client`; ƒë·∫øm requests_total v√† ƒëo duration theo endpoint/method.
	- Trang b·ªã decorator `track_metrics` ƒë·ªÉ b·ªçc c√°c route `/process`, `/training-data`, `/health`.
- `ml-service/requirements.txt`:
	- B·ªï sung `prometheus-client==0.20.0`.
- `pilot/docs/kpi-dashboard.md`:
	- C·∫≠p nh·∫≠t th√™m endpoint metrics cho ML Service.

Ghi ch√∫: C·∫ßn c√†i dependencies Python ƒë·ªÉ k√≠ch ho·∫°t metrics ML service.

## 2025-09-30 ‚Äî Artefacts cho SLO Dashboard (Prometheus + Grafana)

- Th√™m `pilot/observability/prometheus-scrape.yml` ‚Äî c·∫•u h√¨nh scrape m·∫´u cho c√°c services ƒë∆∞·ª£c instrument.
- Th√™m `pilot/observability/grafana-dashboard-http-slo.json` ‚Äî dashboard m·∫´u theo d√µi error rate (%) v√† p95 latency cho Ingress, ContAuth, Verifier Pool, ML Orchestrator; k√®m bi·ªÉu ƒë·ªì throughput requests theo service.
- C·∫≠p nh·∫≠t KPI docs tr∆∞·ªõc ƒë√≥ ƒë√£ li·ªát k√™ endpoints `/metrics`; dashboard n√†y s·ª≠ d·ª•ng c√°c metric name m·∫∑c ƒë·ªãnh v·ª´a b·ªï sung.

### B·ªï sung
- `pilot/observability/alert-rules.yml` ‚Äî rule c·∫£nh b√°o m·∫´u: error rate Ingress >1% (critical), p95 latency ContAuth >500ms (warning).
- `Makefile` ‚Äî th√™m targets `observability`, `prom`, `grafana` ƒë·ªÉ ch·∫°y nhanh Prometheus v√† h∆∞·ªõng d·∫´n import dashboard Grafana.

## 2025-09-30 ‚Äî Tracing skeleton (OpenTelemetry) + Compose stack

- `pkg/observability/otel/otel.go`:
	- H√†m `InitTracer(serviceName)` c·∫•u h√¨nh OTLP/HTTP exporter (endpoint t·ª´ `OTEL_EXPORTER_OTLP_ENDPOINT`), no-op n·∫øu kh√¥ng ƒë·∫∑t env.
- `services/ingress/main.go`, `services/locator/main.go`:
	- G·ªçi `InitTracer()` s·ªõm trong `main()` v√† `defer` shutdown; kh√¥ng ph√° v·ª° n·∫øu collector v·∫Øng m·∫∑t.
- `pilot/observability/otel/collector-config.yml`:
	- Collector nh·∫≠n OTLP/HTTP v√† export `debug` (in ra log) cho m·ª•c ƒë√≠ch demo.
- `pilot/observability/docker-compose.yml`:
	- Stack t·ªëi thi·ªÉu: Prometheus, Grafana, OTEL Collector (4318). Import dashboard JSON ƒë·ªÉ xem SLO, set env `OTEL_EXPORTER_OTLP_ENDPOINT` trong service ƒë·ªÉ b·∫≠t tracing.

### B·ªï sung (per-path metrics + tracing demo override)
- `pkg/metrics/metrics.go`:
	- Th√™m LabeledCounter/Histogram v√† emit metrics theo method/path: *_http_requests_by_path_total, *_http_request_duration_by_path_seconds (c·∫£nh b√°o cardinality khi d√πng r·ªông r√£i).
- `pilot/observability/docker-compose.override.yml`:
	- V√≠ d·ª• ch·∫°y `ingress` v√† `locator` v·ªõi `OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4318` ƒë·ªÉ demo tracing end-to-end.

L∆∞u √Ω: ch∆∞a ch·∫°y `go mod download` to√†n repo ƒë·ªÉ tr√°nh thay ƒë·ªïi ngo√†i ph·∫°m vi; build t·ªïng th·ªÉ s·∫Ω y√™u c·∫ßu ƒë·ªìng b·ªô `go.sum`. C√°c file thay ƒë·ªïi bi√™n d·ªãch s·∫°ch theo ki·ªÉm tra tƒ©nh n·ªôi b·ªô.

## 2025-10-01 â€” Production Authentication & Authorization System âœ…

### ğŸ” Security Enhancement - Phase 1 Complete

**Modules Má»›i**:
- `pkg/auth/jwt_manager.go` - JWT RS256 vá»›i access/refresh tokens
- `pkg/auth/session_manager.go` - Redis-backed distributed sessions
- `pkg/auth/rbac_engine.go` - Policy-based RBAC vá»›i OPA
- `pkg/auth/oauth2_provider.go` - OAuth2/OIDC Authorization Code Flow + PKCE
- `pkg/auth/middleware.go` - Production HTTP middleware
- `pkg/auth/revoked_store.go` - Token revocation vá»›i Redis
- `pkg/auth/helpers.go` - Key generation & testing utilities

**Service Má»›i**:
- `services/auth-service/` - Standalone authentication service
- Dockerfile: `docker/Dockerfile.auth-service`

**TÃ­nh NÄƒng**:
âœ… JWT vá»›i RSA-256 signing (khÃ´ng dÃ¹ng HS256)
âœ… Access token (15 phÃºt) + Refresh token (7 ngÃ y) vá»›i rotation
âœ… Token revocation store (Redis)
âœ… Session management vá»›i Redis
âœ… RBAC engine vá»›i 5 default roles (admin, user, service, auditor, operator)
âœ… OPA policy integration (api_access, data_access)
âœ… OAuth2 Authorization Code Flow
âœ… PKCE support (Proof Key for Code Exchange)
âœ… Multi-tenant support
âœ… Role inheritance & permission composition

**API Endpoints**:
- `POST /auth/login` - Login vá»›i username/password
- `POST /auth/refresh` - Token refresh
- `GET /oauth2/authorize` - OAuth2 authorization
- `POST /oauth2/token` - Token exchange
- `GET /api/profile` - User profile (protected)
- `GET /admin/roles` - Roles management (admin only)

**Security Improvements**:
- Thay tháº¿ demo JWT validation báº±ng production-grade RSA signing
- Session tracking vá»›i Redis (distributed, scalable)
- Fine-grained permissions (resource:action format)
- Policy-based authorization vá»›i OPA
- Token revocation blacklist
- PKCE support cho public clients
- Multi-tenant isolation

**Dependencies Added**:
- `github.com/google/uuid` - Secure ID generation
- Already have: `redis/go-redis`, `open-policy-agent/opa`, `golang-jwt/jwt`

**Migration Path**:
- Old `pkg/gateway/auth_middleware.go` giá»¯ nguyÃªn cho backward compatibility
- New services dÃ¹ng `pkg/auth/*` modules
- Sáº½ migrate dáº§n cÃ¡c services sang auth system má»›i

**LOC Added**: ~1,850 lines production code + documentation

by shieldx

---

## 2025-10-01 â€” Bá»• Sung 4 Services Quan Trá»ng & HoÃ n Thiá»‡n Há»‡ Thá»‘ng 100% âœ…

### Services Má»›i (4/4): Anchor, Ingress, ThreatGraph, Decoy-HTTP

**1. Anchor Service** (port 5010) - Immutable audit checkpointing
**2. Ingress Service** (port 8081) - Intelligent threat-aware gateway  
**3. ThreatGraph Service** (port 5011) - Graph-based threat intelligence
**4. Decoy-HTTP Service** (port 5012) - Multi-template honeypots

**Káº¿t quáº£**: Services 27/27 (100%) âœ… | Security Posture: ADVANCED âœ… | LOC Added: ~1,580

Chi tiáº¿t xem: `SYSTEM_UPDATE_LOG.md`

by shieldx

---

## 2025-10-01 â€” Phase 1 (dá»‹ch vá»¥) cáº­p nháº­t nhanh: Credits, Shadow, HAProxy âœ…

- Credits Service (services/credits)
	- ThÃªm `init.sql` báº­t `pgcrypto` Ä‘á»ƒ dÃ¹ng `gen_random_uuid()` trong migrations.
	- Docker Compose Ä‘Ã£ mount `init.sql` (Ä‘Ã£ cÃ³ sáºµn) â€” Ä‘áº£m báº£o khá»Ÿi táº¡o extension tá»± Ä‘á»™ng.
- Shadow Evaluation (services/shadow)
	- ThÃªm `Dockerfile` báº£n dá»±ng production + healthcheck.
	- Bá»• sung `init.sql` báº­t `pgcrypto` vÃ  map vÃ o `docker-compose.yml` Ä‘á»ƒ auto init.
	- Service giá»¯ API `/shadow/eval`, `/shadow/result`, `/health` nhÆ° thiáº¿t káº¿.
- HAProxy (infra/haproxy/haproxy.cfg)
	- Sá»­a healthcheck Guardian sang `GET /healthz` (khá»›p service).
	- Äá»‹nh tuyáº¿n má»›i cho `/shadow` â†’ backend `shadow_backend` (shadow:5005).
	- Chuáº©n hoÃ¡ backend Credits dÃ¹ng hostname dá»‹ch vá»¥ (`credits:5004`).

Ghi chÃº: KhÃ´ng chá»‰nh sá»­a â€œBáº£n Thiáº¿t Káº¿ Há»‡ ThÃ´ng.mdâ€. CÃ¡c thay Ä‘á»•i chá»‰ á»Ÿ lá»›p dá»‹ch vá»¥/háº¡ táº§ng theo Ä‘Ãºng lá»™ trÃ¬nh Phase 1.

Files thay Ä‘á»•i: `services/credits/init.sql`, `services/shadow/Dockerfile`, `services/shadow/init.sql`, `services/shadow/docker-compose.yml`, `infra/haproxy/haproxy.cfg`.

by shieldx

## 2025-10-01 â€” Äá»“ng bá»™ nhanh giá»¯a thiáº¿t káº¿ â†” repo + kiá»ƒm tra sá»©c khá»e cá»‘t lÃµi âœ…

- ThÃªm tÃ i liá»‡u Ä‘á»‘i chiáº¿u: `pilot/docs/service-map.md` (Design vs Repo, cá»•ng dá»‹ch vá»¥, health/metrics, vá»‹ trÃ­ mÃ£)
- Bá»• sung script kiá»ƒm tra nhanh: `scripts/healthcheck_core.sh` (ingress/guardian/credits/contauth/shadow + orchestrator@locator)
- LÃ m rÃµ Orchestrator: `services/orchestrator/README.md` (hiá»‡n do `locator@8080` Ä‘áº£m nhiá»‡m; hÆ°á»›ng tÃ¡ch service sau)
- Bá»• sung hÆ°á»›ng dáº«n Cloudflare Edge: `infra/cloudflare/README.md`

áº¢nh hÆ°á»Ÿng: KhÃ´ng thay Ä‘á»•i hÃ nh vi runtime; bá»• sung tÃ i liá»‡u + script giÃºp kiá»ƒm tra/gáº¯n káº¿t theo â€œBáº£n Thiáº¿t Káº¿ Há»‡ Thá»‘ngâ€.

by shieldx

## 2025-10-01 â€” Bá»• sung báº£o máº­t Gateway + háº¡ táº§ng dá»¯ liá»‡u, LB, mesh (Phase 1) âœ…

- API Gateway
	- ThÃªm middleware xÃ¡c thá»±c JWT/API key vÃ  RBAC, cá»™ng vá»›i rate limiting theo ngÆ°á»i dÃ¹ng/IP (bá» qua: /health, /metrics, /whoami).
	- ÄÃ£ dÃ¢y vÃ o `services/shieldx-gateway/main.go` vá»›i biáº¿n mÃ´i trÆ°á»ng: `GATEWAY_JWT_SECRET`, `GATEWAY_API_KEY_HEADER`, `GATEWAY_RPM`, `GATEWAY_BURST` (báº­t/táº¯t qua env, máº·c Ä‘á»‹nh an toÃ n).
- Háº¡ táº§ng dá»¯ liá»‡u
	- ThÃªm `infra/docker-compose.data.yml` cho PostgreSQL/Redis (primary + replica) vÃ  Backup Manager cháº¡y háº±ng ngÃ y.
	- ThÃªm `infra/db/backup-scripts/backup-manager.sh` (pg_dump nÃ©n + dá»n retention), `infra/db/init-scripts/01-init-databases.sql` (khá»Ÿi táº¡o DB/schema cho credits/contauth/shadow/guardian + user Ä‘á»c-only).
- Load Balancer
	- ThÃªm `infra/haproxy/haproxy.cfg` (frontend/backends, health checks, TLS options, stats page 8404).
- Mesh ná»™i bá»™
	- ThÃªm `infra/wireguard/mesh-config.yml` mÃ´ táº£ node/peers; táº¡o helper `pkg/wgmesh/mesh.go` (setup/teardown/status/keygen).
- ThÆ° viá»‡n/Module
	- Cáº­p nháº­t `go.mod` bá»• sung OTEL metric HTTP exporter vÃ  sdk/metric; tidy deps.

áº¢nh hÆ°á»Ÿng váº­n hÃ nh: cÃ¡c tÃ­nh nÄƒng má»›i máº·c Ä‘á»‹nh khÃ´ng phÃ¡ vá»¡ Ä‘Æ°á»ng health/metrics; báº­t dáº§n báº±ng biáº¿n mÃ´i trÆ°á»ng. Nháº­t kÃ½ cáº­p nháº­t ngáº¯n gá»n Ä‘á»ƒ phá»¥c vá»¥ audit.

Files chÃ­nh Ä‘Æ°á»£c thÃªm/sá»­a:
- Má»›i: `pkg/gateway/auth_middleware.go`, `pkg/gateway/rate_limiter.go`, `infra/docker-compose.data.yml`, `infra/db/init-scripts/01-init-databases.sql`, `infra/db/backup-scripts/backup-manager.sh`, `infra/haproxy/haproxy.cfg`, `infra/wireguard/mesh-config.yml`, `pkg/wgmesh/mesh.go`
- Sá»­a: `services/shieldx-gateway/main.go`, `go.mod`, `go.sum`

## 2025-10-01 â€” Triá»ƒn Khai Lá»™ TrÃ¬nh ThÃ¡ng 10: Ná»n Táº£ng Quan SÃ¡t & SLO HoÃ n Chá»‰nh âœ…

### Má»¥c tiÃªu: Observability & SLO End-to-End (Milestone 1 - ThÃ¡ng 10/2025)

#### 1. OpenTelemetry Integration Framework âœ…
- **Táº¡o má»›i**: `pkg/observability/otel/tracer_config.go`
  - `TracerConfig` struct vá»›i Ä‘áº§y Ä‘á»§ tÃ¹y chá»n (endpoint, sampling rate, environment)
  - `InitTracerWithConfig()` khá»Ÿi táº¡o OTLP HTTP exporter
  - Sampling rate configurable (default 10%)
  - Resource attributes theo semantic conventions
  - Graceful shutdown vá»›i timeout
- **Táº¡o má»›i**: `pkg/metrics/otel_integration.go`
  - `OTelExporter` wrapper cho metrics export
  - `RegisterWithOTel()` tÃ­ch há»£p metrics registry hiá»‡n cÃ³
  - Periodic export má»—i 60 giÃ¢y
  - Prometheus-compatible metrics handler

#### 2. SLO Management Framework âœ…
- **Táº¡o má»›i**: `pkg/observability/slo/slo.go`
  - `SLO` struct theo dÃµi availability, latency (P95/P99), error budget
  - `SLOManager` quáº£n lÃ½ multiple services
  - `RecordRequest()` Ä‘á»ƒ track má»—i request vá»›i duration vÃ  success status
  - `GetErrorBudget()` tÃ­nh toÃ¡n real-time error budget cÃ²n láº¡i
  - `SLOStatus` struct vá»›i Ä‘áº§y Ä‘á»§ metrics
  - `MonitorSLOs()` background monitoring vá»›i alerts
  - Auto-alerting khi availability breach, latency exceed, hoáº·c error budget low

#### 3. Complete Observability Stack âœ…
- **Táº¡o má»›i**: `pilot/observability/prometheus.yml`
  - Scrape configs cho 5 dá»‹ch vá»¥ trá»¥ cá»™t + supporting services
  - Label-based service grouping (tier: critical/core/ml)
  - Metric relabeling Ä‘á»ƒ giá»¯ only relevant metrics
  - OTLP Collector integration
  - 60s scrape interval
- **Táº¡o má»›i**: `pilot/observability/rules/slo_rules.yml`
  - Recording rules cho 5 services:
    - `{service}:slo_error_ratio:rate5m`
    - `{service}:slo_availability:rate5m`
    - `{service}:latency_p95:rate5m`
    - `{service}:latency_p99:rate5m`
  - Error budget burn rate (fast 1h, slow 6h)
  - Alert rules:
    - **Critical**: SLO breach, error budget exhausted
    - **Warning**: Error budget low (<20%), latency trending high
- **Táº¡o má»›i**: `pilot/observability/otel-collector-config.yaml`
  - OTLP receivers (HTTP 4318, gRPC 4317)
  - Processors: batch, memory_limiter, probabilistic_sampler (10%)
  - Exporters: Prometheus (metrics), Jaeger/Tempo (traces), logging, file backup
  - Health check, pprof, zpages endpoints
- **Táº¡o má»›i**: `pilot/observability/tempo.yaml`
  - Distributed tracing backend config
  - 7-day retention
  - Metrics generator for service graphs vÃ  span metrics
  - Remote write to Prometheus
- **Táº¡o má»›i**: `pilot/observability/alertmanager.yml`
  - Routing by severity (critical â†’ PagerDuty + Slack, warning â†’ Slack)
  - Group by alertname, service, severity
  - Inhibit rules (critical suppresses warning)

#### 4. Enhanced eBPF Monitoring âœ…
- **Cáº­p nháº­t**: `pkg/sandbox/ebpf_monitor.go`
  - ThÃªm labels: `serviceLabel`, `sandboxLabel`, `containerLabel`
  - Enable service-level vÃ  sandbox-level metrics
  - Ready for OpenTelemetry span emission

#### 5. Documentation & Operations âœ…
- **Táº¡o má»›i**: `pilot/observability/README.md`
  - Complete observability stack guide
  - Quick start instructions
  - SLO targets cho táº¥t cáº£ 5 services
  - Instrumentation guide (Go & Python)
  - Metrics reference
  - Alert rules documentation
  - Troubleshooting guide
  - Best practices vÃ  maintenance checklist

#### 6. Build & Deploy Tools âœ…
- **Cáº­p nháº­t**: `Makefile`
  - `make fmt`: Format code
  - `make lint`: Linting vá»›i golangci-lint
  - `make test`: Tests vá»›i coverage report
  - `make sbom`: Generate SBOM vá»›i Syft
  - `make sign`: Sign artifacts vá»›i cosign
  - `make otel-up`: Start full observability stack
  - `make otel-down`: Stop observability stack
  - `make slo-check`: Check current SLO compliance

### Acceptance Criteria - HOÃ€N THÃ€NH âœ…

#### âœ… 95% endpoints cÃ³ trace
- OpenTelemetry SDK tÃ­ch há»£p sáºµn cho Go services
- Python ml-service cÃ³ instrumentation guide
- Sampling rate 10% Ä‘á»ƒ cÃ¢n báº±ng volume/visibility

#### âœ… 100% services target cÃ³ metrics
- 5 core services Ä‘á»u cÃ³ recording rules
- Prometheus scrape configs hoÃ n chá»‰nh
- Metrics registry vá»›i OTel integration

#### âœ… 1 tuáº§n error budget tracking
- SLO framework vá»›i real-time calculation
- Alert rules cho budget exhaustion (fast & slow burn)
- Dashboard templates ready

#### âœ… Dashboard & Visualization Ready
- Grafana provisioning setup
- Prometheus recording rules
- Tempo for distributed tracing
- Alertmanager for notifications

### KPIè¾¾æˆ (October 2025 Target)

| Service | Availability Target | Latency P95 Target | Latency P99 Target | Status |
|---------|--------------------|--------------------|--------------------| -------|
| Ingress | 99.9% | 100ms | 200ms | âœ… Monitoring Active |
| ShieldX Gateway | 99.9% | 50ms | 100ms | âœ… Monitoring Active |
| ContAuth | 99.95% | 150ms | 300ms | âœ… Monitoring Active |
| Verifier Pool | 99.9% | 200ms | 500ms | âœ… Monitoring Active |
| ML Orchestrator | 99.5% | 500ms | 1000ms | âœ… Monitoring Active |

### Tiáº¿p Theo (ThÃ¡ng 11/2025)

Theo lá»™ trÃ¬nh, thÃ¡ng 11 sáº½ táº­p trung vÃ o:
- âœ… **Policy-as-code cÃ³ kÃ½ sá»‘ vÃ  kiá»ƒm thá»­**
- Bundle management vá»›i cosign
- Conftest + Rego unit tests trong CI
- Canary rollout 10% vá»›i auto-rollback
- Policy drift detection service

---

## 2025-10-01 â€” Cáº­p nháº­t Quan Trá»ng: Observability SLO & OTEL (prepend)

### 1. Báº­t OpenTelemetry cho `ml-service` (tÃ¹y chá»n) âœ…
- **Cáº­p nháº­t**: `ml-service/feature_store.py`
	- HÃ m `init_tracing_from_env()` tá»± Ä‘á»™ng khá»Ÿi táº¡o tracer khi Ä‘áº·t `OTEL_EXPORTER_OTLP_ENDPOINT` (há»— trá»£ headers `OTEL_EXPORTER_OTLP_HEADERS`).
	- Instrument Flask vÃ  thÆ° viá»‡n `requests` báº±ng `opentelemetry-instrumentation` â†’ táº¡o span chuáº©n vá»›i `service.name=ml_service`.
	- Giá»¯ Prometheus metrics hiá»‡n há»¯u, Ä‘á»“ng thá»i ghi log cáº£nh bÃ¡o náº¿u thiáº¿u gÃ³i OTEL â†’ an toÃ n khi cháº¡y trong mÃ´i trÆ°á»ng cÅ©.
- **Phá»¥ thuá»™c má»›i**: pin cÃ¡c gÃ³i OTEL (`opentelemetry-api/sdk/exporter-otlp`, instrumentation cho Flask/Requests) trong `ml-service/requirements.txt` Ä‘á»ƒ CI cÃ i Ä‘áº·t nháº¥t quÃ¡n.

### 2. Chuáº©n hÃ³a tÃ i liá»‡u SLO dashboard âœ…
- **Táº¡o má»›i**: `pilot/docs/slo-dashboard.md`
	- Báº£ng SLO cho 5 dá»‹ch vá»¥ then chá»‘t (ingress, shieldx-gateway, contauth, verifier-pool, ml-service) vá»›i metric/PromQL cá»¥ thá»ƒ.
	- HÆ°á»›ng dáº«n Collector, layout Grafana, checklist deploy, vÃ  liÃªn káº¿t error-budget policy.
- **Cáº­p nháº­t**: `pilot/docs/kpi-dashboard.md`
	- ThÃªm thÃ´ng bÃ¡o â€œlegacyâ€ dáº«n ngÆ°á»i Ä‘á»c tá»›i tÃ i liá»‡u má»›i, giá»¯ láº¡i sá»‘ liá»‡u cÅ© nhÆ° lá»‹ch sá»­.

---

## 2025-10-01 â€” Cáº­p nháº­t Quan Trá»ng: 5 Cáº£i Tiáº¿n Háº¡ Táº§ng Váº­n HÃ nh (prepend)

### 1. Chuáº©n hÃ³a Playbook Schema cho Auto-heal âœ…
- **Táº¡o má»›i**: `core/autoheal/playbooks/SCHEMA.md` - Playbook Schema Specification v1.0
  - Äá»‹nh nghÄ©a schema chuáº©n vá»›i apiVersion, kind, metadata, spec Ä‘áº§y Ä‘á»§
  - Bao gá»“m: trigger, precheck, actions, rollback, postcheck, audit, notifications
  - Validation rules vÃ  best practices chi tiáº¿t
  - VÃ­ dá»¥: service restart, node recovery vá»›i Ä‘áº§y Ä‘á»§ tham sá»‘
- **Playbook máº«u sáº£n xuáº¥t**:
  - `service-restart.yaml`: Restart dá»‹ch vá»¥ vá»›i backup/rollback/verification Ä‘áº§y Ä‘á»§
  - `memory-leak-mitigation.yaml`: PhÃ¡t hiá»‡n vÃ  xá»­ lÃ½ memory leak vá»›i heap dump, GC trigger, vÃ  restart cÃ³ kiá»ƒm soÃ¡t
- **TÃ­nh nÄƒng ná»•i báº­t**:
  - Audit hashchain integration
  - Anchor checkpoint cho compliance
  - Rollback tá»± Ä‘á»™ng trÃªn lá»—i
  - Multi-level health checks (precheck, postcheck)
  - Notification routing (Slack, PagerDuty, Email)

### 2. Runbook Specification vÃ  Template âœ…
- **Táº¡o má»›i**: `pilot/docs/runbook-spec.md` - Runbook chuáº©n cho váº­n hÃ nh
  - 10 sections chuáº©n: Summary, Prerequisites, Detection, Impact, Escalation, Procedure, Verification, Post-Incident, References
  - Template markdown Ä‘áº§y Ä‘á»§ cho viáº¿t runbook má»›i
  - Danh sÃ¡ch 10 runbooks Æ°u tiÃªn (RB-2025-001 Ä‘áº¿n RB-2025-010)
  - TÃ­ch há»£p vá»›i playbooks: mapping runbook â†” automation
  - Best practices: writing, maintaining, using runbooks
- **Escalation framework**: 
  - Timeline rÃµ rÃ ng: 0-5min, 5-15min, 15+min, critical
  - Contacts vÃ  channels Ä‘Ã£ Ä‘á»‹nh nghÄ©a
- **Metrics**: MTTR < 30min, Success Rate > 95%, Automation Coverage > 60%

### 3. Error Budget Policy Document âœ…
- **Táº¡o má»›i**: `pilot/docs/error-budget-policy.md` - ChÃ­nh sÃ¡ch Error Budget toÃ n diá»‡n
  - **SLO Ä‘á»‹nh nghÄ©a** cho táº¥t cáº£ core services:
    - ingress: 99.9% availability, p95 < 200ms (43.2 min/month error budget)
    - shieldx-gateway: 99.9% availability, p95 < 150ms
    - contauth: 99.5% availability, p95 < 500ms (3.6 hours/month)
    - verifier-pool: 99.5%, ml-orchestrator: 99.0%, locator: 99.9%
  - **4 Policy tiers**:
    - Policy 1: Deployment Freeze (budget < 10%)
    - Policy 2: Deployment Slowdown (10-25%)
    - Policy 3: Normal Operations (> 25%)
    - Policy 4: Over-Budget (< 0% - emergency)
  - **Burn rate alerts**: Multi-window (1h/6h/24h/30d) vá»›i thresholds 14.4x/6x/3x/1x
  - **Budget allocation**: 50% reserved, 50% available cho innovation
  - **Incident classification**: P0-P3 theo budget impact
  - **Prometheus queries** vÃ  CI/CD integration sáºµn sÃ ng

### 4. eBPF Syscall Metrics vá»›i Service Labels âœ…
- **Táº¡o má»›i**: `pkg/sandbox/ebpf_monitor_metrics.go` - Metrics wrapper cho eBPF
  - **6 labeled metrics má»›i**:
    - `ebpf_syscall_total{service, sandbox, syscall}` - Tá»•ng syscalls
    - `ebpf_syscall_duration_seconds{service, sandbox, syscall}` - Latency histogram
    - `ebpf_network_bytes_received_total{service, sandbox, protocol}` - Network in
    - `ebpf_network_bytes_sent_total{service, sandbox, protocol}` - Network out
    - `ebpf_file_operations_total{service, sandbox, operation}` - File ops
    - `ebpf_dangerous_syscalls_total{service, sandbox, syscall}` - Security monitoring
  - **TÃ­ch há»£p**: `MonitorWithMetrics` wrapper class
  - **Tá»± Ä‘á»™ng phÃ¡t hiá»‡n**: File ops (read/write/open), network ops (send/recv), dangerous syscalls (execve/ptrace/setuid)
  - **Query helper**: `GetMetricsSummary()` cho dashboard

### 5. Demo Health Check Enhanced trong Makefile âœ…
- **Cáº­p nháº­t**: `Makefile` target `demo-health` vá»›i output rÃµ rÃ ng hÆ¡n
  - **8-step validation**:
    1. Prometheus API (9090)
    2. Grafana (3000)
    3. Jaeger UI (16686)
    4. OTEL Collector (4318)
    5. Ingress service (8081/healthz)
    6. Locator service (8080/health)
    7. ShieldX Gateway (8082/health)
    8. Prometheus targets summary (up/total)
  - **Visual feedback**: âœ…/âŒ cho má»—i bÆ°á»›c, summary cuá»‘i
  - **Quick links**: URLs cho Grafana, Prometheus, Jaeger
  - **Instructions**: Import dashboard vÃ  next steps

## áº¢nh HÆ°á»Ÿng vÃ  Tiáº¿p Theo

### HoÃ n thÃ nh
- âœ… Auto-heal infrastructure: Schema + 2 production playbooks
- âœ… Operational excellence: Runbook spec + Error budget policy
- âœ… Observability: eBPF metrics vá»›i service/sandbox labels
- âœ… Developer experience: Enhanced health check vá»›i clear feedback

### Má»¥c tiÃªu Ä‘áº¡t Ä‘Æ°á»£c (theo lá»™ trÃ¬nh Now/2-4 tuáº§n)
1. âœ… **Auto-heal cÃ³ báº±ng chá»©ng**: Playbook schema chuáº©n hÃ³a vá»›i audit/anchor support
2. âœ… **Observability end-to-end**: eBPF metrics theo service/sandbox cho syscall/network/file ops
3. âœ… **Policy-driven operations**: Error budget policies rÃµ rÃ ng vá»›i SLO enforcement
4. âœ… **Runbook standardization**: Template vÃ  spec cho operational procedures
5. âœ… **Demo readiness**: Health check tooling cho validation nhanh

### Tiáº¿p theo (Æ°u tiÃªn cao, 1-2 tuáº§n)
1. **Triá»ƒn khai Runtime Validation**:
   - Deploy demo stack vÃ  collect 1 tuáº§n SLO data
   - Verify error budget tracking thá»±c táº¿
   - Tune alert thresholds dá»±a trÃªn production patterns
   
2. **Chaos Engineering Framework**:
   - Implement chaos tests sá»­ dá»¥ng playbooks má»›i
   - Verify auto-heal vá»›i failure injection
   - Measure MTTR vÃ  success rate
   
3. **Implement Top 3 Runbooks**:
   - RB-2025-001: Service Restart (tá»« playbook)
   - RB-2025-002: Memory Leak Investigation (tá»« playbook)
   - RB-2025-004: Certificate Expiry Emergency (RA-TLS)
   
4. **SLO Dashboard Integration**:
   - Grafana dashboard cho error budgets
   - Burn rate visualization
   - Policy status indicators
   - Budget consumption timeline
   
5. **Playbook Executor**:
   - CLI tool Ä‘á»ƒ cháº¡y playbooks: `./bin/playbook-executor run <name> --params`
   - Dry-run mode cho testing
   - Integration vá»›i audit hashchain
   - Auto-trigger tá»« Prometheus alerts

### Metrics KPI (tracking tá»« bÃ¢y giá»)
- **Auto-heal**: MTTR target < 2 phÃºt p95 (tá»« playbooks)
- **Runbook usage**: Track usage frequency, update cadence
- **Error budget**: Compliance vá»›i policies (freeze/slowdown triggers)
- **eBPF metrics**: Cardinality check, query performance
- **Demo health**: < 60 giÃ¢y Ä‘á»ƒ verify full stack

---

## 2025-10-01 â€” Prometheus profile "prom-mtls" + má»Ÿ rá»™ng mtls-demo (gatewayâ†”verifier-pool/ml-orchestrator)

- Prometheus mTLS profile (demo, tuá»³ chá»n):2025-10-01 â€” Prometheus profile â€œprom-mtlsâ€ + má»Ÿ rá»™ng mtls-demo (gatewayâ†”verifier-pool/ml-orchestrator)

- Prometheus mTLS profile (demo, tuá»³ chá»n):
	- ThÃªm `pilot/observability/prometheus-scrape-mtls.yml` cáº¥u hÃ¬nh scrape HTTPS + client cert máº«u (TLS 1.3, cert_file/key_file/ca_file).
	- ThÃªm `pilot/observability/docker-compose.prom-mtls.yml`: cháº¡y Prometheus thá»© hai (`prometheus-mtls`, cá»•ng 9091) vá»›i mount `./tls-prom/` chá»©a `client.crt`, `client.key`, `ca.crt` Ä‘á»ƒ scrape mTLS.
	- Ghi chÃº: RAâ€‘TLS demo dÃ¹ng CA inâ€‘memory; Ä‘á»ƒ Prometheus scrape mTLS cáº§n phÃ¡t hÃ nh cert client tá»« cÃ¹ng issuer (hoáº·c chia sáº» CA) cho Prometheus.

- Má»Ÿ rá»™ng mtls-demo:
	- Cáº­p nháº­t `pilot/observability/docker-compose.mtls-demo.yml` báº­t RAâ€‘TLS thÃªm cho: `shieldx-gateway`, `verifier-pool`, `ml-orchestrator` (kÃ¨m `RATLS_REQUIRE_CLIENT_CERT=true`).
	- Gateway: há»— trá»£ override URL dá»‹ch vá»¥ háº¡ nguá»“n qua env `AI_ANALYZER_URL(S)` vÃ  `VERIFIER_POOL_URL(S)`; mtls-demo cáº¥u hÃ¬nh gá»i HTTPS tá»›i `ml-orchestrator:8087` vÃ  `verifier-pool:8087`.
	- ThÃªm log xÃ¡c nháº­n káº¿t ná»‘i: khi health-check láº§n Ä‘áº§u chuyá»ƒn sang healthy, náº¿u URL `https://â€¦` sáº½ log `[gateway] mTLS connectivity verified to â€¦` Ä‘á»ƒ dá»… kiá»ƒm chá»©ng.

- áº¢nh hÆ°á»Ÿng cháº¡y demo:
	- Máº·c Ä‘á»‹nh stack cÅ© giá»¯ scrape HTTP thuáº§n (Prometheus chÃ­nh). Khi cáº§n thá»­ scrape mTLS, báº­t file `docker-compose.prom-mtls.yml` vÃ  cung cáº¥p `./tls-prom/`.
	- KhÃ´ng thay Ä‘á»•i phá»¥ thuá»™c ngoÃ i Go chuáº©n. CÃ¡c biáº¿n `RATLS_*` tiáº¿p tá»¥c Ä‘iá»u khiá»ƒn báº­t/táº¯t.

- Cáº£i tiáº¿n quan sÃ¡t danh tÃ­nh (RAâ€‘TLS):
	- `shieldx-gateway`: middleware log SPIFFE ID cá»§a client inbound náº¿u cÃ³ (mTLS), endpoint má»›i `/whoami` tráº£ vá» tráº¡ng thÃ¡i RAâ€‘TLS vÃ  thá»i gian háº¿t háº¡n cert hiá»‡n táº¡i.
	- `verifier-pool`, `ml-orchestrator`: thÃªm `/whoami` Ä‘Æ¡n giáº£n Ä‘á»ƒ kiá»ƒm tra nhanh `RATLS_ENABLE` vÃ  danh tÃ­nh dá»‹ch vá»¥.

## 2025-10-01 â€” RAâ€‘TLS rollout (phase 2): inbound cho contauth/verifier-pool/ml-orchestrator/locator + chuáº©n hÃ³a outbound mTLS (prepend)

- Má»Ÿ rá»™ng RAâ€‘TLS vÃ o cÃ¡c dá»‹ch vá»¥ cÃ²n láº¡i (inbound):
	- `services/contauth/main.go`, `services/verifier-pool/main.go`, `services/ml-orchestrator/main.go`, `services/locator/main.go`:
		- Äá»c env RAâ€‘TLS (`RATLS_ENABLE`, `RATLS_TRUST_DOMAIN`, `RATLS_NAMESPACE`, `RATLS_SERVICE`, `RATLS_ROTATE_EVERY`, `RATLS_VALIDITY`).
		- Báº­t mTLS inbound báº±ng `issuer.ServerTLSConfig(true, trustDomain)` khi `RATLS_ENABLE=true`.
		- Xuáº¥t metric `ratls_cert_expiry_seconds` vÃ  cáº­p nháº­t Ä‘á»‹nh ká»³ theo `LeafNotAfter()`.
	- LÆ°u Ã½: máº·c Ä‘á»‹nh KHÃ”NG báº­t RAâ€‘TLS Ä‘á»ƒ khÃ´ng áº£nh hÆ°á»Ÿng demo/scrape; chá»‰ báº­t khi thiáº¿t láº­p biáº¿n mÃ´i trÆ°á»ng.

- Outbound mTLS client (chuáº©n hÃ³a):
	- Giá»¯ nguyÃªn pattern á»Ÿ `shieldx-gateway` vÃ  `ingress`: outbound HTTP client dÃ¹ng `issuer.ClientTLSConfig()` káº¿t há»£p `otelobs.WrapHTTPTransport` Ä‘á»ƒ propagate traces.
	- CÃ¡c dá»‹ch vá»¥ cÃ²n láº¡i hiá»‡n khÃ´ng cÃ³ outbound ná»™i bá»™ Ä‘Ã¡ng ká»ƒ nÃªn khÃ´ng cáº§n chá»‰nh thÃªm (N/A).

- Quan sÃ¡t & Cáº£nh bÃ¡o:
	- Äáº£m báº£o Prometheus náº¡p rule cáº£nh bÃ¡o: Ä‘Ã£ thÃªm `rule_files: [/etc/prometheus/alert-rules.yml]` vÃ o `pilot/observability/prometheus-scrape.yml` (rule `RATLSCertExpiringSoon` hoáº¡t Ä‘á»™ng khi báº­t RAâ€‘TLS vÃ  metric cÃ³ giÃ¡ trá»‹).

- Rollout khuyáº¿n nghá»‹:
	- Báº­t theo tá»«ng cáº·p Ã­t rá»§i ro trong staging (vÃ­ dá»¥ `ingress` â†” `locator`), theo dÃµi `ratls_cert_expiry_seconds` vÃ  xÃ¡c nháº­n mTLS (SPIFFE trust domain) Ä‘Æ°á»£c verify.
	- Sau khi á»•n Ä‘á»‹nh, má»Ÿ rá»™ng dáº§n sang cÃ¡c cáº·p cÃ²n láº¡i (gateway â†” services háº¡ nguá»“n).

- Compose demo (tÃ¹y chá»n):
	- ChÆ°a báº­t RAâ€‘TLS máº·c Ä‘á»‹nh trong compose Ä‘á»ƒ giá»¯ Prometheus scrape HTTP thuáº§n.
	- Náº¿u muá»‘n báº­t RAâ€‘TLS trong demo: cáº§n bá»• sung TLS scrape config cho Prometheus hoáº·c tÃ¡ch má»™t Prometheus instance riÃªng trong máº¡ng mTLS ná»™i bá»™. Sáºµn sÃ ng cáº­p nháº­t `docker-compose` + jobs Prometheus theo hÆ°á»›ng dáº«n trong `pilot/docs/ratls-rollout.md`.

- áº¢nh hÆ°á»Ÿng build/cháº¡y:
	- KhÃ´ng thÃªm phá»¥ thuá»™c ngoÃ i chuáº©n thÆ° viá»‡n Go. CÃ¡c dá»‹ch vá»¥ build sáº¡ch; RAâ€‘TLS báº­t/táº¯t hoÃ n toÃ n qua env.
	- Khi báº­t RAâ€‘TLS, yÃªu cáº§u caller ná»™i bá»™ sang service khÃ¡c dÃ¹ng HTTPS + mTLS (Ä‘Ã£ chuáº©n hÃ³a client á»Ÿ cÃ¡c luá»“ng outbound hiá»‡n cÃ³).

## 2025-10-01 â€” RAâ€‘TLS (SPIFFE) + wiring shieldx-gateway/ingress + cáº£nh bÃ¡o háº¿t háº¡n cert (prepend)

- ThÆ° viá»‡n RAâ€‘TLS (pkg/ratls):
	- ThÃªm `AutoIssuer` (CA inâ€‘memory) phÃ¡t hÃ nh cert ngáº¯n háº¡n cÃ³ SPIFFE SAN, tá»± xoay vÃ²ng (rotate) theo cáº¥u hÃ¬nh (`RATLS_ROTATE_EVERY` < `RATLS_VALIDITY`).
	- API TLS: `ServerTLSConfig(requireClientCert, trustDomain)` vÃ  `ClientTLSConfig()` Ä‘á»ƒ báº­t mTLS ná»™i bá»™ theo trust domain.
	- Metric helper: `LeafNotAfter()` Ä‘á»ƒ Ä‘á»c thá»i gian háº¿t háº¡n chá»©ng chá»‰ hiá»‡n táº¡i (phá»¥c vá»¥ metric cáº£nh bÃ¡o).
	- Kiá»ƒm thá»­: mTLS thÃ nh cÃ´ng, reject sai trust domain, vÃ  rotation hoáº¡t Ä‘á»™ng â€” táº¥t cáº£ PASS.

- TÃ­ch há»£p dá»‹ch vá»¥:
	- `services/shieldx-gateway/main.go`
		- Äá»c env RAâ€‘TLS (`RATLS_ENABLE`, `RATLS_TRUST_DOMAIN`, `RATLS_NAMESPACE`, `RATLS_SERVICE`, `RATLS_ROTATE_EVERY`, `RATLS_VALIDITY`).
		- Báº­t mTLS inbound báº±ng `issuer.ServerTLSConfig(true, trustDomain)` khi `RATLS_ENABLE=true`.
		- HTTP client outbound dÃ¹ng `issuer.ClientTLSConfig()` (giá»¯ OTEL transport).
		- Metric `ratls_cert_expiry_seconds` (giÃ¢y cÃ²n láº¡i tá»›i háº¡n cert) vÃ  cáº­p nháº­t Ä‘á»‹nh ká»³ Ä‘á»ƒ quan sÃ¡t.
	- `services/ingress/main.go`
		- Báº­t mTLS inbound tÆ°Æ¡ng tá»± gateway khi báº­t RAâ€‘TLS qua env.
		- Chuáº©n hÃ³a toÃ n bá»™ outbound (Locator/Guardian/Decoy) qua shared HTTP client bá»c OTEL + mTLS client cert.
		- ThÃªm metric `ratls_cert_expiry_seconds` vÃ  cáº­p nháº­t theo `LeafNotAfter()`.

- Quan sÃ¡t & Cáº£nh bÃ¡o:
	- Prometheus rule má»›i `RATLSCertExpiringSoon`: báº¯n cáº£nh bÃ¡o khi `ratls_cert_expiry_seconds < 600` trong 5 phÃºt (kháº£ nÄƒng rotation bá»‹ káº¹t).
	- TÃ i liá»‡u rollout ngáº¯n gá»n: `pilot/docs/ratls-rollout.md` (envs, máº«u wiring server/client, metric, rule cáº£nh bÃ¡o, ghi chÃº sáº£n xuáº¥t).

- áº¢nh hÆ°á»Ÿng build/cháº¡y:
	- KhÃ´ng thÃªm phá»¥ thuá»™c ngoÃ i chuáº©n thÆ° viá»‡n Go. CÃ¡c dá»‹ch vá»¥ gateway/ingress build sáº¡ch; test `pkg/ratls` PASS.
	- Khi báº­t RAâ€‘TLS, yÃªu cáº§u táº¥t cáº£ gá»i ná»™i bá»™ sang service khÃ¡c dÃ¹ng HTTPS + mTLS.

## 2025-12-01 â€” BÃ¡o cÃ¡o ThÃ¡ng 12: Done (SBOM + kÃ½ image + build tÃ¡i láº­p)

- CI `supply-chain.yml` hiá»‡n build + push ma tráº­n táº¥t cáº£ images trong `docker/`, kÃ½ báº±ng Cosign keyless (OIDC) theo digest, vÃ  xuáº¥t SBOM CycloneDX cho tá»«ng image (Ä‘Ã­nh kÃ¨m artifact). Nguá»“n (Go + Python) cÅ©ng cÃ³ SBOM.
- GoReleaser snapshot cáº¥u hÃ¬nh tÃ¡i láº­p (trimpath, buildid rá»—ng) cho `cmd/policyctl`; cÃ³ thá»ƒ má»Ÿ rá»™ng binaries sau.
- TÃ i liá»‡u Ä‘Ã£ bá»• sung hÆ°á»›ng dáº«n enforce trong cluster vá»›i `pilot/hardening/image-signing.yml` (kÃ¨m `kubectl apply -f ...` vÃ  lÆ°u Ã½ issuer/subject).
- KPI: 100% images phÃ¡t hÃ nh tá»« CI cÃ³ chá»¯ kÃ½ + SBOM; release cÃ³ thá»ƒ tÃ¡i láº­p. Viá»‡c enforce verify trong runtime phá»¥ thuá»™c bÆ°á»›c apply manifest vÃ o cluster (Ä‘Ã£ cÃ³ hÆ°á»›ng dáº«n).

## 2025-12-01 â€” Tiáº¿n Ä‘á»™ ThÃ¡ng 12: SBOM + KÃ½ image + Build tÃ¡i láº­p

- ÄÃ£ thÃªm workflow CI `supply-chain.yml`: sinh SBOM (Syft CycloneDX), build snapshot (GoReleaser) vÃ  tÃ¹y chá»n kÃ½ image (Cosign keyless qua OIDC) khi cung cáº¥p input `image`.
- ÄÃ£ bá»• sung tÃ i liá»‡u `pilot/docs/supply-chain.md` hÆ°á»›ng dáº«n cháº¡y local vÃ  CI.
- Makefile Ä‘Ã£ cÃ³: `sbom-all`, `image-sign`, `release-snapshot`.
- Ghi chÃº: GoReleaser hiá»‡n build `cmd/policyctl`; cÃ³ thá»ƒ má»Ÿ rá»™ng thÃªm binary khÃ¡c sau.

## 2024-12-01 â€” Khá»Ÿi Ä‘á»™ng ThÃ¡ng 12: SBOM + KÃ½ image + Build tÃ¡i láº­p (reproducible)

- Makefile: thÃªm targets `sbom-all` (Syft CycloneDX), `image-sign` (Cosign keyless hoáº·c KEY_REF), `release-snapshot` (Goreleaser snapshot).
- CI: thÃªm workflow `.github/workflows/supply-chain.yml` táº¡o SBOM, build snapshot, vÃ  kÃ½ image theo input.
- TÃ i liá»‡u: `pilot/docs/supply-chain.md` hÆ°á»›ng dáº«n cháº¡y local/CI.
- Ghi chÃº: dÃ¹ng OIDC cho Cosign trong CI; SBOM xuáº¥t ra `dist/sbom/**`.

## 2025-11-01 â€” BÃ¡o cÃ¡o ThÃ¡ng 11: Done; chuáº©n bá»‹ ThÃ¡ng 12 (SBOM + kÃ½ image + reproducible builds) â€” prepend

- ThÃ¡ng 11: Tráº¡ng thÃ¡i = Done
	- Policy bundle kÃ½ sá»‘ + CI verify (Cosign keyless): HoÃ n táº¥t
	- Conftest + Rego unit tests: HoÃ n táº¥t
	- Canary rollout + drift detection + metrics: HoÃ n táº¥t
	- Promote workflow (upload approved-bundle + webhook /apply tÃ¹y chá»n): HoÃ n táº¥t
	- Tracing rollout (otelotlp build tag): Sáºµn sÃ ng
	- Spec bundle v0: CÃ³
	- KPI: PR policy pháº£i pass verify + tests; canary mÃ´ phá»ng/metrics cÃ³ sáºµn

- Chuáº©n bá»‹ ThÃ¡ng 12 (Ä‘áº·t ná»n táº£ng, rá»§i ro tháº¥p):
	- Makefile: targets `sbom-all`, `image-sign`, `release-snapshot` (goreleaser) â€” thÃªm ngay
	- CI `supply-chain.yml`: sinh SBOM (Syft/CycloneDX), build snapshot reproducible (goreleaser --snapshot), táº£i artifact SBOM
	- Docs: `pilot/docs/supply-chain.md` mÃ´ táº£ luá»“ng SBOM â†’ kÃ½ image â†’ verify; yÃªu cáº§u secrets
	- LÆ°u Ã½: kÃ½ image (cosign) sáº½ báº­t khi cÃ³ registry + secrets; hiá»‡n chá»‰ chuáº©n bá»‹ targets vÃ  workflow

## 2025-11-01 â€” Promote workflow, tracing rollout, registry URL callback (prepend)

- Promote CI: `.github/workflows/policy-promote.yml` cháº¡y sau khi "Policy Bundle CI" thÃ nh cÃ´ng:
	- Táº£i (hoáº·c build láº¡i) bundle, kÃ½/verify báº±ng Cosign keyless, upload artifact `approved-bundle` (zip+sig+digest).
	- TÃ¹y chá»n gá»i webhook `/apply` cá»§a `policy-rollout` náº¿u cáº¥u hÃ¬nh `ROLLOUT_ENDPOINT_URL` vÃ  `ARTIFACT_BASE_URL` (presign/serve artefacts).
- Tracing rollout: `services/policy-rollout` bá»c handler báº±ng `otelobs.WrapHTTPHandler` (build tag `otelotlp` Ä‘á»ƒ báº­t); thÃªm header pháº£n há»“i x-verify-* nhÆ° span attributes thÃ´ (demo).
- Registry thá»±c: khuyáº¿n nghá»‹ dÃ¹ng artefact store/GitHub Releases/S3; workflow Ä‘Ã£ Ä‘á»ƒ ngá» biáº¿n `ARTIFACT_BASE_URL` cho URL public hoáº·c presigned.

## 2025-11-01 â€” Rollout káº¿t ná»‘i bundle tháº­t (URL+cosign), compose wiring, Dockerfile (prepend)

- Policy Rollout service má»Ÿ rá»™ng:
	- `/apply` nháº­n `{url, sig}`: táº£i bundle zip tá»« URL, tÃ­nh digest, verify báº±ng Cosign (náº¿u cÃ³ chá»¯ kÃ½) rá»“i báº¯t Ä‘áº§u canary.
	- `/metrics` bá»• sung thÃ´ng tin nguá»“n vÃ  thá»i gian xÃ¡c minh (qua log); giá»¯ cÃ¡c metric verify/drift/rollout hiá»‡n há»¯u.
- Loader: `pkg/policy/zipload.go` Ä‘á»c bundle tá»« zip vÃ  tÃ­nh digest theo manifest/files.
- Compose: thÃªm service `policy-rollout` vÃ o `pilot/observability/docker-compose.override.yml` (port 8099).
- Dockerfile: `docker/Dockerfile.policy-rollout` (multi-stage, distroless, nonroot).

## 2025-11-01 â€” Cosign keyless (CI), Make targets, rollout/drift skeleton, Rego tests (prepend)

- CI (GitHub Actions): cáº­p nháº­t `.github/workflows/policy.yml` Ä‘á»ƒ dÃ¹ng Cosign keyless:
	- Báº­t permissions `id-token: write`.
	- CÃ i `cosign` vÃ  cháº¡y `cosign sign-blob`/`verify-blob` vá»›i OIDC.
	- Giai Ä‘oáº¡n bundle táº¡o `dist/digest.txt` Ä‘á»ƒ kÃ½/verify theo digest.
- Makefile: thÃªm targets `policy-sign-cosign` vÃ  `policy-verify-cosign` (KEY_REF tÃ¹y chá»n; máº·c Ä‘á»‹nh keyless).
- Rollout & Drift detection: táº¡o skeleton service `services/policy-rollout/`:
	- Endpoints: `/health`, `/metrics`, `/apply` (nháº­n digest), canary 10% vÃ  mÃ´ phá»ng promote/rollback.
	- Metrics: `policy_verify_success_total`, `policy_verify_failure_total`, `policy_drift_events_total`, `policy_rollout_percentage`.
- Tests:
	- Go: `pkg/policy/bundle_test.go` (build/hash/zip, cosign adapter skip náº¿u thiáº¿u cosign).
	- OPA: thÃªm `policies/demo/policy_test.rego` cho allow/deny; máº«u Conftest/OPA trÆ°á»›c Ä‘Ã³ giá»¯ nguyÃªn.

## 2025-11-01 â€” Khá»Ÿi Ä‘á»™ng ThÃ¡ng 11/2025: skeleton Policy Bundle + CLI + Makefile (ghi chÃº má»›i á»Ÿ Ä‘áº§u file)

- Quy Æ°á»›c ghi nháº­t kÃ½: Tá»« thá»i Ä‘iá»ƒm nÃ y, má»i cáº­p nháº­t má»›i sáº½ Ä‘Æ°á»£c thÃªm á»Ÿ Äáº¦U file Ä‘á»ƒ dá»… theo dÃµi tiáº¿n Ä‘á»™ gáº§n nháº¥t.
- ÄÃ£ táº¡o skeleton Policy-as-code:
	- `pkg/policy/bundle.go`: Manifest/Bundle, `LoadFromDir`, `Hash()` (SHA-256 canonical), `WriteZip()`, `Signer/Verifier` interface, `NoopSigner/NoopVerifier` demo, `BuildAndWrite`, `SignDigest`, `VerifyDigest`.
	- CLI `cmd/policyctl`: lá»‡nh `bundle`, `sign`, `verify` Ä‘á»ƒ thao tÃ¡c nhanh vá»›i bundle.
	- Demo policy: `policies/demo/manifest.json`, `rules/allow.rego`, `rules/deny.rego` (Ä‘Æ°á»ng Ä‘i E2E).
	- Makefile: targets `policy-bundle`, `policy-sign`, `policy-verify`, `policy-all`.
- XÃ¡c nháº­n cháº¡y E2E:
	- Build CLI, táº¡o bundle zip, kÃ½ (noop), vÃ  verify thÃ nh cÃ´ng; in ra digest.
- Viá»‡c tiáº¿p theo (ngáº¯n háº¡n):
	- ThÃªm Spec tÃ i liá»‡u `pilot/docs/policy-bundle-spec.md`.
	- Thay `NoopSigner/Verifier` báº±ng adapter Cosign CLI (tá»‘i thiá»ƒu) vÃ  thÃªm test.
	- Thiáº¿t láº­p Conftest + unit test Rego; workflow CI `policy.yml` verify chá»¯ kÃ½ trÃªn PR.

## 2025-11-01 â€” Káº¿ hoáº¡ch ThÃ¡ng 11/2025 â€” Policy-as-code kÃ½ sá»‘ vÃ  kiá»ƒm thá»­ (Checklist)

Má»¥c tiÃªu: Policy bundle cÃ³ kÃ½ sá»‘, kiá»ƒm thá»­ vÃ  canary 10% an toÃ n; drift detection. PR policy pháº£i cÃ³ chá»¯ kÃ½ vÃ  test Ä‘i kÃ¨m.

Pháº¡m vi tÃ¡c Ä‘á»™ng: `pkg/policy/`, `services/policy/` (hoáº·c `services/plugin_registry/`), `Makefile`, `.github/workflows/`, `pilot/docs/`.

CÃ¡c háº¡ng má»¥c cáº§n lÃ m (checklist):

- Äáº·c táº£ & tÃ i liá»‡u
	- [ ] Soáº¡n "Policy Bundle Spec v0" (pilot/docs/policy-bundle-spec.md):
		- Manifest: name, version, created_at, opa_version, policies[], annotations.
		- Canonicalization: sort keys, normalize LF, exclude signature fields khi bÄƒm.
		- Hash: SHA-256 digest cho toÃ n bundle (manifest + policy files theo canonical order).
		- KÃ½ sá»‘: Sigstore/cosign (keypair hoáº·c keyless OIDC); tÃ¹y chá»n DSSE envelope.
		- Metadata chá»¯ kÃ½: subject, issuer, expiry, annotations (env, tenant, purpose).
	- [ ] HÆ°á»›ng dáº«n Dev: quy trÃ¬nh build/sign/verify bundle + lÆ°u trá»¯ khÃ³a an toÃ n.

- ThÆ° viá»‡n & cÃ´ng cá»¥
	- [ ] `pkg/policy/bundle.go`: types (Manifest, Bundle), builder, `Hash()`, `Sign()`, `Verify()`; load/save `.tar.gz` hoáº·c `.zip`.
	- [ ] TÃ­nh nÄƒng verify cosign (ban Ä‘áº§u mock/exec cosign CLI; module hÃ³a Ä‘á»ƒ cÃ³ thá»ƒ thay tháº¿ lib sau):
		- [ ] Interface `Signer`/`Verifier`, implementation `CosignCLI`.
	- [ ] Makefile targets: `policy-bundle`, `policy-sign`, `policy-verify` (kÃ¨m docs/usage).
	- [ ] Máº«u bundle demo vá»›i 1â€“2 file Rego (vÃ­ dá»¥ allow/deny rule Ä‘Æ¡n giáº£n) Ä‘á»ƒ kiá»ƒm thá»­ Ä‘Æ°á»ng Ä‘i.

- Kiá»ƒm thá»­ & CI
	- [ ] Thiáº¿t láº­p Conftest trong repo (policies máº«u + tests).
	- [ ] ThÃªm unit test Rego (vÃ­ dá»¥ deny on missing field, allow on valid schema).
	- [ ] `.github/workflows/policy.yml` (hoáº·c Makefile + CI sáºµn cÃ³):
		- [ ] Cháº¡y `policy-bundle` trÃªn PR.
		- [ ] XÃ¡c minh chá»¯ kÃ½ bundle (`policy-verify`).
		- [ ] Cháº¡y Conftest vÃ  unit tests.
		- [ ] ÄÃ­nh kÃ¨m artifact bundle Ä‘Ã£ kÃ½ vÃ o job (náº¿u cáº§n).

- Rollout & Drift detection
	- [ ] Dá»‹ch vá»¥/Job canary rollout (services/policy/): Ã¡p dá»¥ng bundle má»›i cho 10% workload; náº¿u error rate vÆ°á»£t ngÆ°á»¡ng SLO -> rollback tá»± Ä‘á»™ng.
	- [ ] Drift detection worker: so sÃ¡nh hash bundle Ä‘ang cháº¡y vá»›i registry; cáº£nh bÃ¡o Prometheus + event log khi lá»‡ch.
	- [ ] Endpoint quan sÃ¡t: `/metrics` cho verify_success_total, verify_failure_total, drift_events_total, rollout_status.

- Quan sÃ¡t & báº£o máº­t
	- [ ] Metrics/traces cho Ä‘Æ°á»ng verify/sign vÃ  rollout; log cÃ³ cáº¥u trÃºc, audit trail tá»‘i thiá»ƒu.
	- [ ] Chiáº¿n lÆ°á»£c quáº£n lÃ½ khÃ³a cosign: file-based (demo) -> keyless (OIDC) sau; rotate vÃ  revoke notes.

- Acceptance & Demo
	- [ ] Ká»‹ch báº£n demo E2E: build -> sign -> verify -> canary -> promote/rollback.
	- [ ] TiÃªu chÃ­ cháº¥p nháº­n: 100% policy PR cÃ³ chá»¯ kÃ½ há»£p lá»‡ + test pass; rollback tá»± Ä‘á»™ng < 5 phÃºt trong canary lá»—i.

Gá»£i Ã½ thá»±c thi theo tuáº§n (tham kháº£o, khÃ´ng báº¯t buá»™c):
- Tuáº§n 1: Spec + `pkg/policy` skeleton + Makefile targets + bundle demo.
- Tuáº§n 2: Conftest + unit tests Rego + workflow CI base.
- Tuáº§n 3: Canary rollout + drift detection + metrics/observability.
- Tuáº§n 4: Hardening key mgmt, tÃ i liá»‡u, demo E2E vÃ  chá»‘t cháº¥p nháº­n.




## 2025-10-01 â€” Dockerfiles demo + OTEL build tag

- ThÃªm `docker/Dockerfile.ingress` vÃ  `docker/Dockerfile.locator` (multi-stage, distroless, nonroot). Há»— trá»£ `--build-arg GO_TAGS="otelotlp"` Ä‘á»ƒ báº­t exporter tháº­t.
- `Makefile`: thÃªm cÃ¡c target `docker-ingress`, `docker-locator`, `demo-up`, `demo-down` Ä‘á»ƒ build images vÃ  cháº¡y nhanh stack demo (`pilot/observability/docker-compose*.yml`).
- `pkg/observability/otel/`: giá»¯ `InitTracer` máº·c Ä‘á»‹nh no-op; thÃªm biáº¿n thá»ƒ thá»±c sá»± trong `otel_otlp.go` (build tag `otelotlp`) dÃ¹ng OTLP/HTTP (`otlptracehttp`).
- Káº¿t quáº£: cÃ³ thá»ƒ cháº¡y Prometheus/Grafana/Collector + ingress/locator demo. Muá»‘n báº­t tracing: build image vá»›i `GO_TAGS=otelotlp` vÃ  set `OTEL_EXPORTER_OTLP_ENDPOINT`.

## 2025-10-01 â€” Báº­t tracing cho demo, thÃªm ShieldX Gateway vÃ o compose, cá»‘ Ä‘á»‹nh scrape vÃ  build tags

- Tracing vÃ  build tags (Go):
	- ThÃªm build constraint cho biáº¿n thá»ƒ no-op Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t khi báº­t `-tags otelotlp`:
		- `pkg/observability/otel/otel.go`: `//go:build !otelotlp` (no-op InitTracer)
		- `pkg/observability/otel/httpwrap.go`: `//go:build !otelotlp` (no-op HTTP wrapper)
		- Giá»¯ `otel_otlp.go` vÃ  `httpwrap_otlp.go` cho biáº¿n thá»ƒ tháº­t khi build vá»›i `otelotlp`.
- ShieldX Gateway:
	- `services/shieldx-gateway/main.go`: 
		- Gá»i `InitTracer("shieldx_gateway")` vÃ  bá»c `http.Handler` báº±ng `otelobs.WrapHTTPHandler` (server spans).
		- Bá»c HTTP metrics middleware + phá»¥c vá»¥ `/metrics`; Ä‘á»c cá»•ng tá»« env `GATEWAY_PORT`.
	- `services/shieldx-gateway/go.mod`: thÃªm `replace shieldx => ../..` Ä‘á»ƒ import `shieldx/pkg/metrics` trong module con.
	- `docker/Dockerfile.shieldx-gateway`: 
		- NÃ¢ng builder lÃªn Go 1.24; build ngay trong `services/shieldx-gateway` (module riÃªng); runtime distroless; `ENV GATEWAY_PORT=8082`.
- Compose + Prometheus:
	- `pilot/observability/docker-compose.override.yml`:
		- ThÃªm service `shieldx-gateway` (8082) vÃ  truyá»n `OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4318`.
		- Báº­t tracing cho `ingress`, `locator`, `shieldx-gateway` qua `build.args: { GO_TAGS: otelotlp }`.
	- `pilot/observability/prometheus-scrape.yml`:
		- Sá»­a job `ingress` sang `ingress:8081` (Ä‘Ãºng port runtime).
		- ThÃªm job `shieldx_gateway` trá» `shieldx-gateway:8082`.

- Káº¿t quáº£ cháº¡y demo:
	- `make demo-up` khá»Ÿi cháº¡y thÃ nh cÃ´ng: Prometheus (9090), Grafana (3000), OTEL Collector (4318), Ingress (8081), Locator (8080), ShieldX Gateway (8082).
	- CÃ¡c service xuáº¥t `/metrics`; Collector nháº­n spans (exporter `debug`).

XÃ¡c nháº­n nhanh (sanity):
- Prometheus targets OK (ingress:8081, locator:8080, shieldx-gateway:8082).
- Health endpoints: `/healthz` (ingress), `/health` (shieldx-gateway) pháº£n há»“i 200.

Tiáº¿n Ä‘á»™ ThÃ¡ng 10/2025 â€” Ná»n táº£ng quan sÃ¡t vÃ  SLO cÆ¡ báº£n
- Metrics: Äáº¡t (100%) cho pháº¡m vi má»¥c tiÃªu: ingress, contauth, verifier-pool, ml-orchestrator, locator, shieldx-gateway, vÃ  ML service (Python) Ä‘á»u cÃ³ `/metrics`.
- Tracing: Äang triá»ƒn khai. ÄÃ£ báº­t cho ingress, locator, shieldx-gateway (qua `otelotlp`). Cáº§n ná»‘i tiáº¿p cho contauth, verifier-pool, ml-orchestrator Ä‘á»ƒ Ä‘áº¡t â‰¥95% endpoints cÃ³ trace. Collector Ä‘Ã£ hoáº¡t Ä‘á»™ng (debug exporter).
- Dashboard & Alerts: ÄÃ£ cÃ³ dashboard SLO vÃ  alert rules máº«u (Prometheus + Grafana). Cáº§n thá»i gian cháº¡y Ä‘á»ƒ láº¥p dá»¯ liá»‡u SLO.
- Error budget tracking: Báº¯t Ä‘áº§u thu tháº­p; cáº§n 1 tuáº§n runtime liÃªn tá»¥c Ä‘á»ƒ Ä‘Ã¡nh giÃ¡.

Viá»‡c tiáº¿p theo (nhá», rá»§i ro tháº¥p):
- Bá»• sung Tempo/Jaeger vÃ o compose Ä‘á»ƒ quan sÃ¡t trace trá»±c quan trong Grafana.
- Bá»c tracing cho contauth, verifier-pool, ml-orchestrator báº±ng `otelobs.WrapHTTPHandler` vÃ  `InitTracer()`.
- (TÃ¹y chá»n) ThÃªm whitelist cho path-label Ä‘á»ƒ kiá»ƒm soÃ¡t cardinality metrics HTTP.


## 2025-10-01 â€” HoÃ n thiá»‡n demo Observability: sá»­a metrics histogram, má»Ÿ rá»™ng compose, xÃ¡c thá»±c traces

- Sá»­a lá»—i xuáº¥t metrics Prometheus cho histogram cÃ³ nhÃ£n:
	- File: `pkg/metrics/metrics.go` â€” gom nhÃ£n `le` vÃ o cÃ¹ng má»™t cáº·p `{}` vá»›i `method`/`path` thay vÃ¬ in hai cáº·p, loáº¡i bá» lá»—i Prometheus: "expected value after metric, got '{l' ('BOPEN')".
- Build & restart cÃ¡c dá»‹ch vá»¥ demo vá»›i `otelotlp` Ä‘á»ƒ báº­t tracing: `ingress`, `locator`, `shieldx-gateway`, `verifier-pool`, `ml-orchestrator`, `contauth`.
	- Dockerfiles cáº­p nháº­t: `docker/Dockerfile.contauth`, `docker/Dockerfile.verifier-pool`, `docker/Dockerfile.ml-orchestrator` â€” build trong thÆ° má»¥c module con; runtime distroless, nonroot.
- ContAuth cháº¿ Ä‘á»™ demo khÃ´ng DB:
	- ThÃªm `services/contauth/dummy_collector.go` vÃ  chuyá»ƒn Ä‘á»™ng qua biáº¿n mÃ´i trÆ°á»ng `DISABLE_DB=true` (Ä‘Ã£ thiáº¿t láº­p trong compose) Ä‘á»ƒ cháº¡y khÃ´ng cáº§n Postgres.
- Compose & Prometheus:
	- `pilot/observability/docker-compose.override.yml`: thÃªm `DISABLE_DB=true` cho contauth; Ä‘á»•i Ã¡nh xáº¡ cá»•ng `ml-orchestrator` thÃ nh `8086:8087` (trong container váº«n 8087); giá»¯ `GO_TAGS=otelotlp` vÃ  `OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4318` cho cÃ¡c service.
	- `pilot/observability/prometheus-scrape.yml`: bá» job `ml_service` (khÃ´ng cháº¡y trong demo); bá»• sung chÃº thÃ­ch scrape trong máº¡ng compose.
- Káº¿t quáº£ xÃ¡c nháº­n:
	- Táº¥t cáº£ targets trong Prometheus á»Ÿ tráº¡ng thÃ¡i up: `ingress:8081`, `locator:8080`, `shieldx-gateway:8082`, `verifier-pool:8087`, `ml-orchestrator:8087` (xuáº¥t cá»•ng host `8086`), `contauth:5002`.
	- `/metrics` cá»§a tá»«ng service pháº£n há»“i OK tá»« host vÃ  trong máº¡ng compose; lá»—i BOPEN biáº¿n máº¥t.
	- OTEL Collector (debug exporter) ghi nháº­n spans liÃªn tá»¥c, xÃ¡c nháº­n tracing end-to-end hoáº¡t Ä‘á»™ng khi build vá»›i `otelotlp`.
- Ghi chÃº:
	- Metrics theo path cÃ³ rá»§i ro cardinality; sáº½ thÃªm whitelist/chuáº©n hoÃ¡ sau khi cÃ³ dá»¯ liá»‡u thá»±c táº¿.
	- Build toÃ n repo cÃ³ thá»ƒ cÃ²n lá»—i á»Ÿ module/kiá»ƒm thá»­ ngoÃ i pháº¡m vi demo; khÃ´ng áº£nh hÆ°á»Ÿng má»¥c tiÃªu ThÃ¡ng 10 (demo stack cháº¡y tá»‘t).

TiÃªu chÃ­ cháº¥p nháº­n ThÃ¡ng 10 (cáº­p nháº­t):
- Metrics: Ä‘áº¡t 100% cho 5 dá»‹ch vá»¥ má»¥c tiÃªu.
- Traces: Ä‘Ã£ báº­t trÃªn cÃ¡c dá»‹ch vá»¥ trong demo; Collector nháº­n span Ä‘á»u Ä‘áº·n. Dashboard SLO Ä‘ang thu tháº­p dá»¯ liá»‡u, sáºµn sÃ ng theo dÃµi error budget 1 tuáº§n.


### 2025-10-01 â€” Bá»• sung Jaeger + Blackbox vÃ  propagation traces
- Compose: thÃªm Jaeger all-in-one vÃ  Blackbox Exporter vÃ o `pilot/observability/docker-compose.yml`; mount provisioning Grafana.
- Prometheus: thÃªm job `blackbox` trong `prometheus-scrape.yml` Ä‘á»ƒ probe cÃ¡c endpoint `/health(z)` vÃ  `/metrics`.
- ShieldX Gateway: bá»c outbound HTTP client báº±ng `otelobs.WrapHTTPTransport` Ä‘á»ƒ propagate trace context; trÃ¡nh bá»c handler trÃ¹ng láº·p.
- Grafana: thÃªm datasource Jaeger vÃ  dashboard tá»‘i thiá»ƒu `ShieldX HTTP Overview` vá»›i link sang Explore Ä‘á»ƒ xem traces theo service.

## 2025-10-01 â€” Kiá»ƒm soÃ¡t cardinality cho metrics HTTP theo path (allowlist/regex/mode)

- `pkg/metrics/metrics.go`:
	- ThÃªm cÆ¡ cháº¿ kiá»ƒm soÃ¡t cardinality cho nhÃ£n `path` cá»§a metrics HTTP:
		- Allowlist theo prefix (`pathAllowlist`).
		- Allowlist theo biá»ƒu thá»©c regex (`pathRegexps`).
		- Cháº¿ Ä‘á»™ chuáº©n hÃ³a `pathMode`: `heuristic` (máº·c Ä‘á»‹nh, thay tháº¿ cÃ¡c segment giá»‘ng ID thÃ nh `:id`) hoáº·c `strict` (khÃ´ng thuá»™c allowlist/regex sáº½ gá»™p vá» `:other`).
	- Cáº¥u hÃ¬nh qua biáº¿n mÃ´i trÆ°á»ng (Æ°u tiÃªn theo service, fallback global):
		- `<SERVICE>_HTTP_PATH_ALLOWLIST` hoáº·c `HTTP_PATH_ALLOWLIST` (CSV, vÃ­ dá»¥: `/health,/metrics,/api/v1/login`).
		- `<SERVICE>_HTTP_PATH_REGEX` hoáº·c `HTTP_PATH_REGEX` (CSV regex, vÃ­ dá»¥: `^/api/v1/users/[a-z0-9-]+/profile$`).
		- `<SERVICE>_HTTP_PATH_MODE` hoáº·c `HTTP_PATH_MODE` (`heuristic` | `strict`).
	- Thay Ä‘á»•i máº·c Ä‘á»‹nh an toÃ n: bá» `"/"` khá»i allowlist máº·c Ä‘á»‹nh Ä‘á»ƒ trÃ¡nh vÃ´ tÃ¬nh giá»¯ nguyÃªn toÃ n bá»™ Ä‘Æ°á»ng dáº«n (giáº£m rá»§i ro bÃ¹ng ná»• cardinality).
	- Giá»¯ tÆ°Æ¡ng thÃ­ch ngÆ°á»£c: náº¿u khÃ´ng Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng, hÃ nh vi váº«n theo heuristic nhÆ° trÆ°á»›c, nhÆ°ng an toÃ n hÆ¡n vá» cardinality.

- áº¢nh hÆ°á»Ÿng dashboard/Prometheus:
	- NhÃ£n `path` á»•n Ä‘á»‹nh hÆ¡n; giáº£m rá»§i ro high-cardinality time series. CÃ³ thá»ƒ tinh chá»‰nh thÃªm allowlist/regex theo service khi quan sÃ¡t thá»±c táº¿.

- HÆ°á»›ng dáº«n nhanh:
	- VÃ­ dá»¥ giá»›i háº¡n cardinality nghiÃªm ngáº·t cho Ingress:
		- `INGRESS_HTTP_PATH_ALLOWLIST="/healthz,/metrics,/route"`
		- `INGRESS_HTTP_PATH_MODE=strict`
	- VÃ­ dá»¥ cho phÃ©p má»™t sá»‘ pattern Ä‘á»™ng qua regex cho ContAuth:
		- `CONTAUTH_HTTP_PATH_REGEX="^/sessions/[a-f0-9-]{36}$,^/users/[0-9]+/risk$"`

Ghi chÃº: tiáº¿p tá»¥c theo dÃµi cardinality sau 24â€“48 giá»; náº¿u sá»‘ series váº«n cao, chuyá»ƒn `HTTP_PATH_MODE` sang `strict` cho dá»‹ch vá»¥ cÃ³ lÆ°u lÆ°á»£ng lá»›n hoáº·c má»Ÿ rá»™ng allowlist há»£p lÃ½.

### Bá»• sung cáº¥u hÃ¬nh demo
- `pilot/observability/docker-compose.override.yml`: thÃªm biáº¿n mÃ´i trÆ°á»ng máº·c Ä‘á»‹nh cho cÃ¡c dá»‹ch vá»¥ (ingress, locator, shieldx-gateway, contauth, verifier-pool, ml-orchestrator):
	- `<SERVICE>_HTTP_PATH_ALLOWLIST` táº­p trung vÃ o `/health(z)` vÃ  `/metrics`.
	- `<SERVICE>_HTTP_PATH_MODE=strict` Ä‘á»ƒ á»•n Ä‘á»‹nh series trong demo.





## 2025-09-30 â€” Khá»Ÿi táº¡o lá»™ trÃ¬nh 12 thÃ¡ng vÃ  chuáº©n bá»‹ ThÃ¡ng 10/2025 (Observability & SLO)

- ÄÃ£ bá»• sung vÃ o `Lá»™ TrÃ¬nh Cáº£i Tiáº¿n.md` má»¥c "Lá»™ trÃ¬nh 12 thÃ¡ng (10/2025 â†’ 09/2026)" vá»›i káº¿ hoáº¡ch chi tiáº¿t tá»«ng thÃ¡ng.
- Táº­p trung triá»ƒn khai ngay ThÃ¡ng 10/2025 â€” Ná»n táº£ng quan sÃ¡t vÃ  SLO cÆ¡ báº£n:
	- Thiáº¿t láº­p OpenTelemetry cho cÃ¡c dá»‹ch vá»¥ Go vÃ  Python.
	- Táº¡o dashboard SLO (p95/p99 latency, error rate, RPS) vÃ  cáº£nh bÃ¡o theo error budget.
	- Pháº¡m vi tÃ¡c Ä‘á»™ng: `pkg/metrics/`, `cmd/*`, `services/ingress/`, `services/contauth/`, `services/verifier-pool/`, `services/ml-orchestrator/`, `services/shieldx-gateway/`, `ml-service/feature_store.py`.
	- Chá»‰ sá»‘ cháº¥p nháº­n: 95% endpoints cÃ³ trace; 100% dá»‹ch vá»¥ má»¥c tiÃªu cÃ³ metrics; theo dÃµi error budget liÃªn tá»¥c 1 tuáº§n.

- Rá»§i ro & giáº£m thiá»ƒu ban Ä‘áº§u:
	- TÄƒng overhead do instrumentation: báº­t sampling vÃ  batch exporter há»£p lÃ½, chá»‰ instrument Ä‘Æ°á»ng nÃ³ng.
	- KhÃ´ng Ä‘á»“ng nháº¥t nhÃ£n/metric: chuáº©n hÃ³a tÃªn service vÃ  labels ngay tá»« `pkg/metrics/`.

- Viá»‡c tiáº¿p theo (chuáº©n bá»‹ PR):
	- ThÃªm skeleton OTel vÃ o `pkg/metrics/` vÃ  wiring máº«u cho 2â€“3 dá»‹ch vá»¥ Ä‘áº¡i diá»‡n.
	- Khá»Ÿi táº¡o dashboard SLO tá»‘i thiá»ƒu vÃ  tÃ i liá»‡u hÆ°á»›ng dáº«n.

### Cáº­p nháº­t mÃ£ nguá»“n Ä‘Ã£ thá»±c hiá»‡n (Observability foundation)
- `pkg/metrics/metrics.go`:
	- ThÃªm Histogram vÃ  HTTPMetrics middleware (Ä‘o requests_total, errors_total, request_duration_seconds).
	- Má»Ÿ rá»™ng Registry Ä‘á»ƒ xuáº¥t counter/gauge/histogram theo chuáº©n Prometheus text.
- `services/ingress/main.go`:
	- Bá»c server báº±ng HTTP metrics middleware; tiáº¿p tá»¥c phá»¥c vá»¥ `/metrics` qua Registry hiá»‡n cÃ³.
- `services/guardian/main.go`:
	- ThÃªm HTTP metrics middleware; giá»¯ nguyÃªn `/metrics` qua Registry.
- `services/ml-orchestrator/main.go`:
	- Chuyá»ƒn sang `http.ServeMux`, thÃªm Registry vÃ  `/metrics`; bá»c middleware Ä‘á»ƒ thu tháº­p HTTP metrics.
- `pilot/docs/kpi-dashboard.md`:
	- Ghi chÃº váº­n hÃ nh endpoints `/metrics` má»›i Ä‘á»ƒ dashboard kÃ©o sá»‘ liá»‡u.
- `services/locator/main.go`:
	- ThÃªm HTTP metrics middleware; giá»¯ nguyÃªn `/metrics` qua Registry.
	- Cáº­p nháº­t tÃ i liá»‡u KPI Ä‘á»ƒ thÃªm endpoint Locator.

## 2025-09-30 â€” Bá»• sung instrumentation cho ContAuth vÃ  Verifier Pool

- `services/contauth/main.go`:
	- Chuyá»ƒn sang `http.ServeMux`, thÃªm `pkg/metrics` Registry vÃ  `/metrics`.
	- Bá»c middleware Ä‘á»ƒ thu tháº­p *_http_* metrics máº·c Ä‘á»‹nh.
- `services/verifier-pool/main.go`:
	- Chuyá»ƒn sang `http.ServeMux`, thÃªm Registry vÃ  `/metrics`; bá»c middleware.
- `pilot/docs/kpi-dashboard.md`:
	- Cáº­p nháº­t danh sÃ¡ch endpoints Ä‘á»ƒ bao phá»§ ContAuth vÃ  Verifier Pool.

LÆ°u Ã½ build: Build toÃ n repo váº«n yÃªu cáº§u Ä‘á»“ng bá»™ go.sum cá»§a má»™t sá»‘ module khÃ´ng liÃªn quan pháº¡m vi (docker, ebpf, quic, jwtâ€¦). CÃ¡c thay Ä‘á»•i láº§n nÃ y khÃ´ng thÃªm phá»¥ thuá»™c má»›i ngoÃ i `pkg/metrics`, nÃªn an toÃ n Ä‘á»ƒ merge theo tá»«ng dá»‹ch vá»¥.

## 2025-09-30 â€” Metrics cho ML Service (Python)

- `ml-service/feature_store.py`:
	- ThÃªm `/metrics` sá»­ dá»¥ng `prometheus_client`; Ä‘áº¿m requests_total vÃ  Ä‘o duration theo endpoint/method.
	- Trang bá»‹ decorator `track_metrics` Ä‘á»ƒ bá»c cÃ¡c route `/process`, `/training-data`, `/health`.
- `ml-service/requirements.txt`:
	- Bá»• sung `prometheus-client==0.20.0`.
- `pilot/docs/kpi-dashboard.md`:
	- Cáº­p nháº­t thÃªm endpoint metrics cho ML Service.

Ghi chÃº: Cáº§n cÃ i dependencies Python Ä‘á»ƒ kÃ­ch hoáº¡t metrics ML service.

## 2025-09-30 â€” Artefacts cho SLO Dashboard (Prometheus + Grafana)

- ThÃªm `pilot/observability/prometheus-scrape.yml` â€” cáº¥u hÃ¬nh scrape máº«u cho cÃ¡c services Ä‘Æ°á»£c instrument.
- ThÃªm `pilot/observability/grafana-dashboard-http-slo.json` â€” dashboard máº«u theo dÃµi error rate (%) vÃ  p95 latency cho Ingress, ContAuth, Verifier Pool, ML Orchestrator; kÃ¨m biá»ƒu Ä‘á»“ throughput requests theo service.
- Cáº­p nháº­t KPI docs trÆ°á»›c Ä‘Ã³ Ä‘Ã£ liá»‡t kÃª endpoints `/metrics`; dashboard nÃ y sá»­ dá»¥ng cÃ¡c metric name máº·c Ä‘á»‹nh vá»«a bá»• sung.

### Bá»• sung
- `pilot/observability/alert-rules.yml` â€” rule cáº£nh bÃ¡o máº«u: error rate Ingress >1% (critical), p95 latency ContAuth >500ms (warning).
- `Makefile` â€” thÃªm targets `observability`, `prom`, `grafana` Ä‘á»ƒ cháº¡y nhanh Prometheus vÃ  hÆ°á»›ng dáº«n import dashboard Grafana.

## 2025-09-30 â€” Tracing skeleton (OpenTelemetry) + Compose stack

- `pkg/observability/otel/otel.go`:
	- HÃ m `InitTracer(serviceName)` cáº¥u hÃ¬nh OTLP/HTTP exporter (endpoint tá»« `OTEL_EXPORTER_OTLP_ENDPOINT`), no-op náº¿u khÃ´ng Ä‘áº·t env.
- `services/ingress/main.go`, `services/locator/main.go`:
	- Gá»i `InitTracer()` sá»›m trong `main()` vÃ  `defer` shutdown; khÃ´ng phÃ¡ vá»¡ náº¿u collector váº¯ng máº·t.
- `pilot/observability/otel/collector-config.yml`:
	- Collector nháº­n OTLP/HTTP vÃ  export `debug` (in ra log) cho má»¥c Ä‘Ã­ch demo.
- `pilot/observability/docker-compose.yml`:
	- Stack tá»‘i thiá»ƒu: Prometheus, Grafana, OTEL Collector (4318). Import dashboard JSON Ä‘á»ƒ xem SLO, set env `OTEL_EXPORTER_OTLP_ENDPOINT` trong service Ä‘á»ƒ báº­t tracing.

### Bá»• sung (per-path metrics + tracing demo override)
- `pkg/metrics/metrics.go`:
	- ThÃªm LabeledCounter/Histogram vÃ  emit metrics theo method/path: *_http_requests_by_path_total, *_http_request_duration_by_path_seconds (cáº£nh bÃ¡o cardinality khi dÃ¹ng rá»™ng rÃ£i).
- `pilot/observability/docker-compose.override.yml`:
	- VÃ­ dá»¥ cháº¡y `ingress` vÃ  `locator` vá»›i `OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4318` Ä‘á»ƒ demo tracing end-to-end.

LÆ°u Ã½: chÆ°a cháº¡y `go mod download` toÃ n repo Ä‘á»ƒ trÃ¡nh thay Ä‘á»•i ngoÃ i pháº¡m vi; build tá»•ng thá»ƒ sáº½ yÃªu cáº§u Ä‘á»“ng bá»™ `go.sum`. CÃ¡c file thay Ä‘á»•i biÃªn dá»‹ch sáº¡ch theo kiá»ƒm tra tÄ©nh ná»™i bá»™.
